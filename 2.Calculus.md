# Calculus

# Derivatives

## Machine Learning Motivation

- Calculus and Derivatives are important in machine learning because derivatives are used to optimize functions in particular to maximize them and minimize them.

![Untitled](Images/Calculus/Untitled%200.png)

## Motivation to Derivatives

- The concept of derivative is similar to velocity, a car that goes 100km/hr means that it’s average velocity is 100 km/hr. However you started fast, then went slow, This velocity is not constant.
- At a given instant, what is the velocity at that instant? That's called the **instantaneous velocity**. That is precisely what a **derivative** is.
- A **derivative** is the instantaneous rate of change of a function, In this case, the function is distance and its derivative is velocity
- You can find the average velocity in the interval from 10-15, but you don't know what happened within that interval at time 12.5 exactly.
- The average velocity between the time interval 10-15 seconds is also the **slope** of the **line** that joins the two points in the graph.
- **Velocity** is calculated with the formula, distance over time. This is **synonymous** to the formula for calculating **slope**, which is **rise** over **run**.

![Untitled](Images/Calculus/Untitled%201.png)

- we can have a good estimate for the velocity if we had more data about the distances of time close to t equals 12.5 seconds.

![Untitled](Images/Calculus/Untitled%202.png)

- Notice that we still don't have the estimate of the velocity at 12.5. To find this estimate, We need to take finer and finer intervals and the finer the intervals, the better the estimate is. That leads to the **derivative.**

## Derivatives and Tangents

- Calculating the instantaneous velocity here may be hard, but estimating it is possible.
- let's take another point to the right of **t** equals 12.5 and calculate the average velocity over that interval. That is the slope of this line over here,

![Untitled](Images/Calculus/Untitled%203.png)

- However that was not the instantaneous velocity at t equals 12.5, We can get closer to it by making this interval smaller.

![Untitled](Images/Calculus/Untitled%204.png)

- Imagine putting it so close that you can't even tell the distance between the two.
- You get the limit which is  $\frac {dx}{dt}$  and that is precisely the **tangent line** to the curve at **t** equals 12.5.
- This measure of how fast the distance is changing with respect to time is called the instantaneous rate of change and it is the **slope** of that **tangent line**.
- The **derivative** of a function at a point is precisely the **slope** of the **tangent** at that particular point.

![Untitled](Images/Calculus/Untitled%205.png)

## Slopes, Maxima and Minima

![Untitled](Images/Calculus/Untitled%206.png)

- At any of the points where the tangent is horizontal, therefore, the tangent has Slope 0.

![Untitled](Images/Calculus/Untitled%207.png)

- Notice that the point where the distance was the farthest is coincidentally one of the points where the car was stopped.
- This is no coincidence because if the car is moving, then it could just go further.
- The point where the car is the farthest is a point where the car is not moving.
- That means if you want to find the **maximum** or the **minimum** in a function, it occurs at one of the points where the derivative is **zero**. Or in other words, where the tangent line is **horizontal**.

## Derivatives and their Notation

![Untitled](Images/Calculus/Untitled%208.png)

## Some Common Derivatives - Lines

![Untitled](Images/Calculus/Untitled%209.png)

![Untitled](Images/Calculus/Untitled%2010.png)

## Some Common Derivatives - Quadratics

![Untitled](Images/Calculus/Untitled%2011.png)

## Some Common Derivatives - Higher Degree Polynomials

![Untitled](Images/Calculus/Untitled%2012.png)

## Some Common Derivatives - Other Power Functions

![Untitled](Images/Calculus/Untitled%2013.png)

![Untitled](Images/Calculus/Untitled%2014.png)

## The Inverse Function and its Derivative

![Untitled](Images/Calculus/Untitled%2015.png)

## Derivative of Trigonometric Functions

![Untitled](Images/Calculus/Untitled%2016.png)

![Untitled](Images/Calculus/Untitled%2017.png)

![Untitled](Images/Calculus/Untitled%2018.png)

![Untitled](Images/Calculus/Untitled%2019.png)

## Meaning of the Exponential $(e)$

![Untitled](Images/Calculus/Untitled%2020.png)

![Untitled](Images/Calculus/Untitled%2021.png)

## The Derivate of $e^x$

![Untitled](Images/Calculus/Untitled%2022.png)

![Untitled](Images/Calculus/Untitled%2023.png)

## The Derivative of $log(x)$

![Untitled](Images/Calculus/Untitled%2024.png)

![Untitled](Images/Calculus/Untitled%2025.png)

## Existence of the Derivative

![Untitled](Images/Calculus/Untitled%2026.png)

![Untitled](Images/Calculus/Untitled%2027.png)

![Untitled](Images/Calculus/Untitled%2028.png)

![Untitled](Images/Calculus/Untitled%2029.png)

## Properties of the Derivative: Multiplication by Scalars

![Untitled](Images/Calculus/Untitled%2030.png)

![Untitled](Images/Calculus/Untitled%2031.png)

## Properties of the Derivative: The Sum Rule

![Untitled](Images/Calculus/Untitled%2032.png)

## Properties of the Derivative: The Product Rule

![Untitled](Images/Calculus/Untitled%2033.png)

![Untitled](Images/Calculus/Untitled%2034.png)

## Properties of the Derivative: The Chain Rule

![Untitled](Images/Calculus/Untitled%2035.png)

![Untitled](Images/Calculus/Untitled%2036.png)

![Untitled](Images/Calculus/Untitled%2037.png)

# Optimization

## Introduction to Optimization

- Main application in machine learning of derivatives is that they are used for **optimization**.
- **Optimization** is when you want to find the maximum or the minimum value of a function.

![Untitled](Images/Calculus/Untitled%2038.png)

- A very interesting property of **maximum** and **minimum** points. The **slope** at those points is always **zero**.

![Untitled](Images/Calculus/Untitled%2039.png)

- It's not always true that a point with slope **zero** is a **maximum** or a **minimum**.
- These **candidates** for the **minimum** are the points of **zero** slope and those are called **local** minima, and the absolute **minimum** is called the **global minimum**.

![Untitled](Images/Calculus/Untitled%2040.png)

## Optimization of Squared Loss - The Two Powerline Problem

![Untitled](Images/Calculus/Untitled%2041.png)

- let's visualize the cost as the area of a square, To identify where we should put the house.

![Untitled](Images/Calculus/Untitled%2042.png)

![Untitled](Images/Calculus/Untitled%2043.png)

- Looks like middle is the best spot to put your house.

![Untitled](Images/Calculus/Untitled%2044.png)

- Using mathematics to find the best spot:
    - The optimal solution is to put the house at a middle point between a and b.

![Untitled](Images/Calculus/Untitled%2045.png)

## Optimization of Squared Loss - The Three Powerline Problem

![Untitled](Images/Calculus/Untitled%2046.png)

![Untitled](Images/Calculus/Untitled%2047.png)

![Untitled](Images/Calculus/Untitled%2048.png)

## Optimization of Log-Loss

- What to do if you want to find the best coin in order to win a game of coin toss?
    - What we have to do is find the optimal value of **p**, the one that helps us maximize the **probability** of winning.
    - In our case, The probability of winning is to land the coin in heads seven times, and to land it in tails three times.
    
    ![Untitled](Images/Calculus/Untitled%2049.png)
    
    - To maximize $G(p)$, We can take the derivative and set it equal to zero.
    - The three candidates for the optimal point are $p=0, p=1$  and $p=0.7$
    - If p equals 0, then the coin will always land in tails and you will have no chance of landing in heads seven times. That one is discarded.
    - The second one is discarded as well, because if p equals 1, then the coin always lands in heads and it has no chance of falling in tails three times.
    - Therefore, the answer is p equals 0.7. Indeed, coin 1 was the best possible coin one could pick.

![Untitled](Images/Calculus/Untitled%2050.png)

- There an easier way to do it by taking the **logarithm** of $G(p)$
- The trick of taking a **logarithm** is a pretty popular trick in machine learning.
- As a matter of fact, what we really want is $-G(p)$ and That is called the **log loss** and it's a very useful loss function in machine learning. It is used a lot in classification problems.
- The reason that we actually take $-G(p)$ instead of $G(p)$ is that **logarithm** of **p** is actually a negative **number** when **p** is in between **zero** and **one**. We want $-G(p)$ to be a positive number, and instead of maximizing it, like we did here, we **minimize** $-G(p)$.

![Untitled](Images/Calculus/Untitled%2051.png)

- Another reason why we use logarithms is because the product of tiny things is tiny. Imagine if you have a probability that is the product of 1,000 probabilities. That's the product of 1,000 numbers, between 0 and 1. That number could be really small and maybe a computer is not able to handle a number so small. On the other hand, if it's the logarithm, the logarithm of a very small number is a very large negative number and computers can handle those numbers.

![Untitled](Images/Calculus/Untitled%2052.png)