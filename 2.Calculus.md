# Calculus

# Derivatives

## Machine Learning Motivation

- Calculus and Derivatives are important in machine learning because derivatives are used to optimize functions in particular to maximize them and minimize them.

![Untitled](Images/Calculus/Untitled%200.png)

## Motivation to Derivatives

- The concept of derivative is similar to velocity, a car that goes 100km/hr means that it’s average velocity is 100 km/hr. However you started fast, then went slow, This velocity is not constant.
- At a given instant, what is the velocity at that instant? That's called the **instantaneous velocity**. That is precisely what a **derivative** is.
- A **derivative** is the instantaneous rate of change of a function, In this case, the function is distance and its derivative is velocity
- You can find the average velocity in the interval from 10-15, but you don't know what happened within that interval at time 12.5 exactly.
- The average velocity between the time interval 10-15 seconds is also the **slope** of the **line** that joins the two points in the graph.
- **Velocity** is calculated with the formula, distance over time. This is **synonymous** to the formula for calculating **slope**, which is **rise** over **run**.

![Untitled](Images/Calculus/Untitled%201.png)

- we can have a good estimate for the velocity if we had more data about the distances of time close to t equals 12.5 seconds.

![Untitled](Images/Calculus/Untitled%202.png)

- Notice that we still don't have the estimate of the velocity at 12.5. To find this estimate, We need to take finer and finer intervals and the finer the intervals, the better the estimate is. That leads to the **derivative.**

## Derivatives and Tangents

- Calculating the instantaneous velocity here may be hard, but estimating it is possible.
- let's take another point to the right of **t** equals 12.5 and calculate the average velocity over that interval. That is the slope of this line over here,

![Untitled](Images/Calculus/Untitled%203.png)

- However that was not the instantaneous velocity at t equals 12.5, We can get closer to it by making this interval smaller.

![Untitled](Images/Calculus/Untitled%204.png)

- Imagine putting it so close that you can't even tell the distance between the two.
- You get the limit which is  $\frac {dx}{dt}$  and that is precisely the **tangent line** to the curve at **t** equals 12.5.
- This measure of how fast the distance is changing with respect to time is called the instantaneous rate of change and it is the **slope** of that **tangent line**.
- The **derivative** of a function at a point is precisely the **slope** of the **tangent** at that particular point.

![Untitled](Images/Calculus/Untitled%205.png)

## Slopes, Maxima and Minima

![Untitled](Images/Calculus/Untitled%206.png)

- At any of the points where the tangent is horizontal, therefore, the tangent has Slope 0.

![Untitled](Images/Calculus/Untitled%207.png)

- Notice that the point where the distance was the farthest is coincidentally one of the points where the car was stopped.
- This is no coincidence because if the car is moving, then it could just go further.
- The point where the car is the farthest is a point where the car is not moving.
- That means if you want to find the **maximum** or the **minimum** in a function, it occurs at one of the points where the derivative is **zero**. Or in other words, where the tangent line is **horizontal**.

## Derivatives and their Notation

![Untitled](Images/Calculus/Untitled%208.png)

## Some Common Derivatives - Lines

![Untitled](Images/Calculus/Untitled%209.png)

![Untitled](Images/Calculus/Untitled%2010.png)

## Some Common Derivatives - Quadratics

![Untitled](Images/Calculus/Untitled%2011.png)

## Some Common Derivatives - Higher Degree Polynomials

![Untitled](Images/Calculus/Untitled%2012.png)

## Some Common Derivatives - Other Power Functions

![Untitled](Images/Calculus/Untitled%2013.png)

![Untitled](Images/Calculus/Untitled%2014.png)

## The Inverse Function and its Derivative

![Untitled](Images/Calculus/Untitled%2015.png)

## Derivative of Trigonometric Functions

![Untitled](Images/Calculus/Untitled%2016.png)

![Untitled](Images/Calculus/Untitled%2017.png)

![Untitled](Images/Calculus/Untitled%2018.png)

![Untitled](Images/Calculus/Untitled%2019.png)

## Meaning of the Exponential $(e)$

![Untitled](Images/Calculus/Untitled%2020.png)

![Untitled](Images/Calculus/Untitled%2021.png)

## The Derivate of $e^x$

![Untitled](Images/Calculus/Untitled%2022.png)

![Untitled](Images/Calculus/Untitled%2023.png)

## The Derivative of $log(x)$

![Untitled](Images/Calculus/Untitled%2024.png)

![Untitled](Images/Calculus/Untitled%2025.png)

## Existence of the Derivative

![Untitled](Images/Calculus/Untitled%2026.png)

![Untitled](Images/Calculus/Untitled%2027.png)

![Untitled](Images/Calculus/Untitled%2028.png)

![Untitled](Images/Calculus/Untitled%2029.png)

## Properties of the Derivative: Multiplication by Scalars

![Untitled](Images/Calculus/Untitled%2030.png)

![Untitled](Images/Calculus/Untitled%2031.png)

## Properties of the Derivative: The Sum Rule

![Untitled](Images/Calculus/Untitled%2032.png)

## Properties of the Derivative: The Product Rule

![Untitled](Images/Calculus/Untitled%2033.png)

![Untitled](Images/Calculus/Untitled%2034.png)

## Properties of the Derivative: The Chain Rule

![Untitled](Images/Calculus/Untitled%2035.png)

![Untitled](Images/Calculus/Untitled%2036.png)

![Untitled](Images/Calculus/Untitled%2037.png)

# Optimization

## Introduction to Optimization

- Main application in machine learning of derivatives is that they are used for **optimization**.
- **Optimization** is when you want to find the maximum or the minimum value of a function.

![Untitled](Images/Calculus/Untitled%2038.png)

- A very interesting property of **maximum** and **minimum** points. The **slope** at those points is always **zero**.

![Untitled](Images/Calculus/Untitled%2039.png)

- It's not always true that a point with slope **zero** is a **maximum** or a **minimum**.
- These **candidates** for the **minimum** are the points of **zero** slope and those are called **local** minima, and the absolute **minimum** is called the **global minimum**.

![Untitled](Images/Calculus/Untitled%2040.png)

## Optimization of Squared Loss - The Two Powerline Problem

![Untitled](Images/Calculus/Untitled%2041.png)

- let's visualize the cost as the area of a square, To identify where we should put the house.

![Untitled](Images/Calculus/Untitled%2042.png)

![Untitled](Images/Calculus/Untitled%2043.png)

- Looks like middle is the best spot to put your house.

![Untitled](Images/Calculus/Untitled%2044.png)

- Using mathematics to find the best spot:
    - The optimal solution is to put the house at a middle point between a and b.

![Untitled](Images/Calculus/Untitled%2045.png)

## Optimization of Squared Loss - The Three Powerline Problem

![Untitled](Images/Calculus/Untitled%2046.png)

![Untitled](Images/Calculus/Untitled%2047.png)

![Untitled](Images/Calculus/Untitled%2048.png)

## Optimization of Log-Loss

- What to do if you want to find the best coin in order to win a game of coin toss?
    - What we have to do is find the optimal value of **p**, the one that helps us maximize the **probability** of winning.
    - In our case, The probability of winning is to land the coin in heads seven times, and to land it in tails three times.
    
    ![Untitled](Images/Calculus/Untitled%2049.png)
    
    - To maximize $G(p)$, We can take the derivative and set it equal to zero.
    - The three candidates for the optimal point are $p=0, p=1$  and $p=0.7$
    - If p equals 0, then the coin will always land in tails and you will have no chance of landing in heads seven times. That one is discarded.
    - The second one is discarded as well, because if p equals 1, then the coin always lands in heads and it has no chance of falling in tails three times.
    - Therefore, the answer is p equals 0.7. Indeed, coin 1 was the best possible coin one could pick.

![Untitled](Images/Calculus/Untitled%2050.png)

- There an easier way to do it by taking the **logarithm** of $G(p)$
- The trick of taking a **logarithm** is a pretty popular trick in machine learning.
- As a matter of fact, what we really want is $-G(p)$ and That is called the **log loss** and it's a very useful loss function in machine learning. It is used a lot in classification problems.
- The reason that we actually take $-G(p)$ instead of $G(p)$ is that **logarithm** of **p** is actually a negative **number** when **p** is in between **zero** and **one**. We want $-G(p)$ to be a positive number, and instead of maximizing it, like we did here, we **minimize** $-G(p)$.

![Untitled](Images/Calculus/Untitled%2051.png)

- Another reason why we use logarithms is because the product of tiny things is tiny. Imagine if you have a probability that is the product of 1,000 probabilities. That's the product of 1,000 numbers, between 0 and 1. That number could be really small and maybe a computer is not able to handle a number so small. On the other hand, if it's the logarithm, the logarithm of a very small number is a very large negative number and computers can handle those numbers.

![Untitled](Images/Calculus/Untitled%2052.png)

# Gradients

## Introduction to Tangent Planes

- On the left graph, We learned how to identify the derivative at each point as the **slope** of the **tangent line** of the point.
- On the right graph, We have a function with 2 inputs and 1 output, It need to be plotted in 3 dimensions, In this case the tangents are not lines. They're planes. That is called the **tangent plane.**
- How do we get this tangent plane?
    - We have to cut the space into planes and then calculate the tangents on these planes.
    - First, We will cut the plane by Fixing y and calculate the derivative of x, Then cut the plane by fixing x and calculate the derivative of y.
    - Once we have the two tangent lines, then we have a plane. Because any two lines that cross represent a unique plane.
    
    ![Untitled](Images/Calculus/Untitled%2053.png)
    

## Partial Derivatives

- If you have a function of two variables plotted in 3D space and you cut it with a plane like this, you get this red parabola with a tangent over there, that's the partial derivative.

![Untitled](Images/Calculus/Untitled%2054.png)

![Untitled](Images/Calculus/Untitled%2055.png)

![Untitled](Images/Calculus/Untitled%2056.png)

![Untitled](Images/Calculus/Untitled%2057.png)

![Untitled](Images/Calculus/Untitled%2058.png)

![Untitled](Images/Calculus/Untitled%2059.png)

![Untitled](Images/Calculus/Untitled%2060.png)

## Gradients

- The **gradient** is simply the vector containing the **partial derivatives.**

![Untitled](Images/Calculus/Untitled%2061.png)

![Untitled](Images/Calculus/Untitled%2062.png)

## Gradients and Maxima/Minima

- The **gradient** is pretty useful to **minimize** functions of two or more variables in the same way as the derivative was useful to minimize functions of one variable.
- Notice that the tangent plane where the minimum located is parallel to the floor, and that's going to happen in every local minimum and maximum.
- To find the **minimums** and **maximums**, all you have to do is set all the **partial derivatives** to 0 and solve that system of equations.

![Untitled](Images/Calculus/Untitled%2063.png)

## Optimization with Gradients: An Example

![Untitled](Images/Calculus/Untitled%2064.png)

![Untitled](Images/Calculus/Untitled%2065.png)

![Untitled](Images/Calculus/Untitled%2066.png)

## Optimization using Gradients - Analytical Method

![Untitled](Images/Calculus/Untitled%2067.png)

![Untitled](Images/Calculus/Untitled%2068.png)

![Untitled](Images/Calculus/Untitled%2069.png)

![Untitled](Images/Calculus/Untitled%2070.png)

# Gradient Descent

## Optimization using Gradient Descent in One Variable

- that these problems can get really complicated really fast, especially in higher dimensions. In this lesson, I'm going to show you a method that is iterative and that , the method is called
- **Gradient descent** is a powerful for **minimizing or maximizing** functions, especially in higher dimensions.
- For some functions, It is very hard to solve analytically like the one below:

![Untitled](Images/Calculus/Untitled%2071.png)

- That takes us to gradient descent
- Here is one method to solve the previous problem.
    - Pick some random value.
    - Move to the left by a little bit, and move to the right by a little bit.
    - Figure out which of these two points is better
    - Go for the one on the right, which has a smaller value in the function.
    - Repeat previous steps until the both points on the two sides have a higher value.
    - Therefore, let's settle for this point. We say that this point it may not be the minimum, but it's close enough to the minimum.
    
    ![Untitled](Images/Calculus/Untitled%2072.png)
    
- Here is another method to solve the previous problem with a clever idea:
    - After picking a random value, instead of calculating the left and the right points.
    - You will subtract the slope of the current point Because:
        - if you are at a point where the **slope** is **negative**, you need to **add** a little bit to the coordinate of the point.
        - Whereas if you're in a place where the **slope** is **positive**, then you need to **subtract** a little bit to the coordinate of the point.
    - In short, if you want a new point that is **closer** to the **minimum**, then that should be the old point minus the slope Because:
        - if you're to the **left** of the new **minimum**, the slope is **negative**. You're subtracting a **negative** number and moving to the **right**.
        - If you were to the **right** of the **minimum**, then you **subtract** a **positive** number, the **slope**, and move to the **left**.
    
    ![Untitled](Images/Calculus/Untitled%2073.png)
    
    - However, there is a small caveat:
        - Imagine that you're at a very steep part of the curve, because the function is steep, then the derivative is very large.
        - You're making a really long jump. Long jumps can be pretty chaotic.
        - The reason is because you may actually **miss** the **minimum** and go pretty far away and get lost.
        - We want to take **small steps** and to be more secure in our path.
        - To take into account a small step, We simply modify the formula and put a small number, multiplying the slope, and that's called the **learning rate** and is denoted **Alpha**.
    
    ![Untitled](Images/Calculus/Untitled%2074.png)
    
    - This formula has an added bonus:
        - If you're  far away from the **local minimum**, and in a steep part of the curve. You'll be taking a relatively big step, obviously not big because you don't want to miss the point and go too far.
        - However, if you're really close to the minimum in a part that's more flat, you want to take a small step.
        - This is included in the formula because if you're in a steep part of the curve, the derivative is big, and you're taking a large step.
        - However, if you're in a flat land, then the derivative is small and you're taking a small step.
        - This method is called **gradient descent** and it is really useful in machine learning.
    
    ![Untitled](Images/Calculus/Untitled%2075.png)
    
    ![Untitled](Images/Calculus/Untitled%2076.png)
    
- Notice that in this algorithm, You never need to solve for the derivative zero. You only do know the derivative and then apply it in the algorithm when you're taking the updating step. That's a huge improvement.

![Untitled](Images/Calculus/Untitled%2077.png)

- What is a good **Learning Rate?**
    - If you have a learning rate that's too large, then you may miss the minimum and never be able to find it because your steps are just too big.
    - If the learning rate is too small then you may take forever to actually reach the minimum or perhaps never reach it.
    - What you want is a learning rate that is not too large that you may miss the minimum but not too small that takes you forever to reach the minimum.
    - There are a lot of really good methods that change the learning rate based on how the problem is doing, but there's no definite method to find a good learning rate.
    
    ![Untitled](Images/Calculus/Untitled%2078.png)
    
- There's another problem with gradient descent may have.
    - Some function have more than one minimum, multiple local minimums and one global minimum.
    - Depends on where you initialize your point or the learning rate, you might get into one of the local minimums.
    
    ![Untitled](Images/Calculus/Untitled%2079.png)
    
    - There's actually no secure way to overcome it. But a way to get pretty good results is to run the gradient descent algorithm many times with many different starting points. Chances are one of them will get you to the minimum or at least to a pretty good point.
    
    ![Untitled](Images/Calculus/Untitled%2080.png)
    

## Optimization using Gradient Descent in Two Variable

![Untitled](Images/Calculus/Untitled%2081.png)

- Notice that this is exactly the same as **gradient descent** in one variable, except instead of the **derivative**, we simply take the **gradient**.

![Untitled](Images/Calculus/Untitled%2082.png)

![Untitled](Images/Calculus/Untitled%2083.png)

![Untitled](Images/Calculus/Untitled%2084.png)

![Untitled](Images/Calculus/Untitled%2085.png)

![Untitled](Images/Calculus/Untitled%2086.png)

- Just like gradient descent in one variable gradient descent in two or more variables still has the same drawbacks.

![Untitled](Images/Calculus/Untitled%2087.png)

## Optimization using Gradient Descent - Least Squares

![Untitled](Images/Calculus/Untitled%2088.png)

![Untitled](Images/Calculus/Untitled%2089.png)

## Optimization using Gradient Descent - Least Squares with Multiple Observations

![Untitled](Images/Calculus/Untitled%2090.png)

![Untitled](Images/Calculus/Untitled%2091.png)