# Probability and Statistics

# Introduction to Probability

## What is Probability?

- **Probability** is a measure of how likely an event is to occur.
- We can calculate the **probability** by dividing the number of favorable outcomes by the total number of possible outcomes.

![Untitled](Images/ProbabilityandStatistics/Untitled%200.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%201.png)

- In probability, an **experiment** is any process that produces an outcome that is uncertain.

![Untitled](Images/ProbabilityandStatistics/Untitled%202.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%203.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%204.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%205.png)

## Complement of Probability

- The **complement** of an event is the probability that the event does not occur.

![Untitled](Images/ProbabilityandStatistics/Untitled%206.png)

- The **complement** rule states that the probability of an event A not occurring is equal to 1 minus the probability of A occurring.

![Untitled](Images/ProbabilityandStatistics/Untitled%207.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%208.png)

## Sum of Probabilities(Disjoint Functions)

- Calculating the sum of probabilities, Considering the Events are **disjoint.**
- **Disjoint events** are events that never occur at the same time. These are also known as **mutually exclusive** events.
- **Disjoint events** are typically represented by the **Union**$(\bigcup)$ .

![Untitled](Images/ProbabilityandStatistics/Untitled%209.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2010.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2011.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2012.png)

## Sum of Probabilities(Joint Functions)

- **Joint events** are events that occur simultaneously or together. In other words, they are events that have outcomes in common.
- **Joint events** are typically represented by the **intersection $(\bigcap)$.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2013.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2014.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2015.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2016.png)

## Independence

- **Independence** happens when the occurrence of one event does not affect the probability of the occurrence of another event.
- When we want the intersection of two events and those events are independent of each other, then it's the **product** of the two **probabilities.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2017.png)

- The **product rule** says probability of A **intersection** B is probability of A times the probability of B, Given that the probabilities are **independent**.

![Untitled](Images/ProbabilityandStatistics/Untitled%2018.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2019.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2020.png)

## Birthday Problem

![Untitled](Images/ProbabilityandStatistics/Untitled%2021.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2022.png)

## Conditional Probability

- **Conditional probability** is about calculating the probability of an event happening given that another event has already happened.

![Untitled](Images/ProbabilityandStatistics/Untitled%2023.png)

- Recall that the **product rule** for **independent** events said that the probability of **A intersection B** is **P(A) * P(B)**. But this only happen when **A** and **B** were **independent**.
- If we have events that are **not independent.** The probability of **A intersection B**, Is the probability of **A** times the probability **B given A.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2024.png)

- So This is the **general product rule.**
- Notice that when events are independent, the **probability of B given A** is the same as **probability of B** because **A** doesn't make any difference on the occurrence of **B.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2025.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2026.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2027.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2028.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2029.png)

## Bayes Theorem - Intuition

- **Bayes theorem** is one of the most important theorems in probability and is used all over the place, including in machine learning. It is used for spam recognition, speech detection, and many other things.

![Untitled](Images/ProbabilityandStatistics/Untitled%2030.png)

- Since this disease happens to one out of every 10,000, If you have a population of 1 million, Then 999,900 are healthy and 100 are sick.

![Untitled](Images/ProbabilityandStatistics/Untitled%2031.png)

- If we test every single person and the test is 99% effective:
    - Among the healthy people it made a mistake in 1% of them which is 9,999.(Healthy people diagnosed as sick)
    - The other 99% of healthy people were correctly diagnosed as healthy.
    - For the sick people it correctly diagnosed 99 people as sick, Since the test is 99% effective.
    - Also it misdiagnosed 1 person as healthy while the person is sick.

![Untitled](Images/ProbabilityandStatistics/Untitled%2032.png)

- We want to figure out the probability that you're sick, given that you were diagnosed sick.
- Since most of the people who diagnosed as sick are mostly healthy, Ypu get a probability of less than 1%

![Untitled](Images/ProbabilityandStatistics/Untitled%2033.png)

- So you get diagnosed sick, but the probability that you’re sick is less than 1%.
- What we saw there is an example of **Bayes Theorem.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2034.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2035.png)

## Bayes Theorem - Mathematical Formula

![Untitled](Images/ProbabilityandStatistics/Untitled%2036.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2037.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2038.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2039.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2040.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2041.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2042.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2043.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2044.png)

## Bayes Theorem - Spam Example

![Untitled](Images/ProbabilityandStatistics/Untitled%2045.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2046.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2047.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2048.png)

## Bayes Theorem - Prior and Posterior

- **Prior** is the original probability that you can calculate, not knowing anything else.
- Then something happens, and that's the event.
- The event gives you information about the probability. With that information, you can calculate something called the **posterior**.

![Untitled](Images/ProbabilityandStatistics/Untitled%2049.png)

- The **posterior** is always a better estimation than the prior because we have that event that gave us information.

![Untitled](Images/ProbabilityandStatistics/Untitled%2050.png)

## Bayes Theorem - The Naive Bayes Model

- What if we want to build a classifier with more than one word, more than one event.

![Untitled](Images/ProbabilityandStatistics/Untitled%2051.png)

- To calculate the probability that an email is spam given the fact that it contains lottery and winning using Bayes theorem.
- you will encounter a problem which is you’d be calculating the number of spam emails that contain the two words divided by the total number of spam emails.
- Say instead we want find spam email with 100 words, So using **Bayes theorem** I need to divide the number of emails that contain all 100 words divided by the number of spam emails.
- However, asking for an email to contain 100 words is very hard. Maybe there's no emails like that in our database, and so we get 0 over 0. And that's not a good calculation.

![Untitled](Images/ProbabilityandStatistics/Untitled%2052.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2053.png)

- The solution is instead of calculating the precise probability, We’re going to estimate it using the **Naive assumption.**
- **Naive assumption** says that the appearances of the words lottery and winning are **independent** from each other.
- This is obviously not true, Words are not independent from each other, But if we assume that they're independent, then the math works out much better.
- If you look at probability of lottery and winning given spam, that's the probability of an intersection of two things, And if they happen to be **independent**, then their probability is the **product** of the two things.

![Untitled](Images/ProbabilityandStatistics/Untitled%2054.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2055.png)

- This can happen with **N words**. **N** can be huge, but the probability of an email containing all these words given that is spam can be estimated as the **product** of all these probabilities of the email containing each one of the words given that it's spam. And that is the **naïve Bayes algorithm**.

![Untitled](Images/ProbabilityandStatistics/Untitled%2056.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2057.png)

## Probability in Machine Learning

- It turns out that machine learning is a lot about probabilities. Many times in machine learning what you want to do is, you want to calculate a probability of something given some other factors.

![Untitled](Images/ProbabilityandStatistics/Untitled%2058.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2059.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2060.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2061.png)

# Probability Distributions

## Random Variables

- Random variables can take many values. For example, the temperature is a random variable that can take many values. Another random variable is the number of heads I obtain if I toss a coin 10 times.
- You can think of X as a variable that not always has the same value.

![Untitled](Images/ProbabilityandStatistics/Untitled%2062.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2063.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2064.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2065.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2066.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2067.png)

## Probability Distributions(Discrete)

- Imagine if we have all the possible scenarios of an event that can happen. Put them on this horizontal axis and for each one of them, look at the probability that they happen. This forms a **probability distribution.**
- These are the probabilities that can be visualized by a histogram.

![Untitled](Images/ProbabilityandStatistics/Untitled%2068.png)

- All discrete random variables can be modeled by their **probability mass function**, also abbreviated as **PMF**.
- It contains all the necessary information to understand how the probability distributes among all the possible values of the variable.

![Untitled](Images/ProbabilityandStatistics/Untitled%2069.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2070.png)

## Binomial Distributions

- The **binomial distribution** is an example of a discrete distributions
- Say we toss a coin 10 times. How many heads can I obtain? It could be from 0 all the way up to 10. Each one comes with its own corresponding probability, so we draw the histogram of probabilities, and that's the **binomial distribution**.
- What is the probability to obtain two heads when you flip five coins?
    - For each flip, there's a 1/2 chance to get heads or tails. If you multiply these together, you get 1/32, which is the probability of this specific outcome.
    - But there are more ways to obtain two heads out of five, Also it has the same probability as the above one.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2071.png)
    
    - It turns out that there are actually 10 possibilities to get two heads out of five.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2072.png)
    
    - There are some general way to find the number of possible combinations.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2073.png)
    
    - This is called the **binomial coefficient**, and it counts the number of ways you can order two heads and three tails.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2074.png)
    
    - One common property of binomial coefficient is that $\dbinom n k$ is the same as$\dbinom {n} {n-k}$. The reason for this is because obtaining **k** heads is the same thing as obtaining **n-k** tails.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2075.png)
    
    - Here is a general way to write the **PMF** for the number of heads in **n** coin tosses:
        - We will multiply the probability of seeing **x heads** times probability of seeing **n - x tails,** This is the probability of one particular ordering.
        - To take into account all possible orders which is the **binomial coefficient** $\dbinom n k$
        
        ![Untitled](Images/ProbabilityandStatistics/Untitled%2076.png)
        
        - We write the equation like that:
        - The tilde symbol means that the variable X follows the distribution to the right of the expression.
        
        ![Untitled](Images/ProbabilityandStatistics/Untitled%2077.png)
        
        ![Untitled](Images/ProbabilityandStatistics/Untitled%2078.png)
        
    - If **p** is **0.5** and **n = 5**, then you get the following **PMF**.
    - Notice that since **p** is **1/2**, then the **PMF** is **symmetrical.**
        
        ![Untitled](Images/ProbabilityandStatistics/Untitled%2079.png)
        
    - If we have a biased coin say of **p=0.3,** then you have greater chances of seeing a small number of heads, and this is reflected in the **PMF**. ****

![Untitled](Images/ProbabilityandStatistics/Untitled%2080.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2081.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2082.png)

## Binomial coefficient

- Binomial coefficient is the number of ways to obtain **k** elements out of a set of **n** in an **unordered way**.
- So in order to get the binomial coefficient, First we need to get the ordered set of length **k.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2083.png)

- Since we know many of the combinations actually repeat, how many of them repeat?
    - We had that the number of ways of picking **ordered sets of length k** that picked every set many times.
    - **K!** is simply the number of ways of ordering **k** numbers or **k** different objects.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2084.png)
    
    - So since we want them **unordered**, then we have to divide by **k!**
- That is the expression for $\dbinom n k$ for the binomial coefficient $\dbinom n k$, the number of ways to obtain **k** elements out of a set of **n** in an **unordered** way.

![Untitled](Images/ProbabilityandStatistics/Untitled%2085.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2086.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2087.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2088.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2089.png)

## Bernoulli Distribution

- **Bernoulli** is a very important distribution in probability and statistics, which has one parameter (p), Which is the probability of **success**.

![Untitled](Images/ProbabilityandStatistics/Untitled%2090.png)

## Probability distributions(Continuous)

- When your events are a list, you have a **discrete distribution**. And when your events are an **interval**, you have a **continuous distribution.**
- So with a continuous problem instead of asking what is the probability at specific time, You should ask what's the probability between a certain window time.

![Untitled](Images/ProbabilityandStatistics/Untitled%2091.png)

- This is how a continuous distribution looks like, It is just a bunch of very, very skinny bars, infinitely many of them, that become just a curve.

![Untitled](Images/ProbabilityandStatistics/Untitled%2092.png)

## Probability Density Function

- **Discrete distributions** have a **probability mass function**, on the other hand, **Continuous distributions** have a **probability density function.**
- In **discrete distributions** probabilities responds to **height**, the more the height the more the probability.
- In **continuous distributions** probabilities respond to areas, the thicker the interval, the more probability it is.

![Untitled](Images/ProbabilityandStatistics/Untitled%2093.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2094.png)

- If we want the probability of calls at exactly 2 minutes, then we'll be talking about the area of this segment over here, that is a line segment, and it has no area. So, that's why we say that that probability is 0.

![Untitled](Images/ProbabilityandStatistics/Untitled%2095.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2096.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2097.png)

## Cumulative Distribution Function

- In **continuous distribution** you have to calculate areas underneath a curve in order to find the probability that a call is between, let's say, two and three minutes.
- The **cumulative distribution** function  shows the actual probability that the call is between zero and a certain number of minutes.
- In order to get **cumulative distribution**, we need to get the **cumulative probability,** and it's this red curve over here. Notice that it always starts at zero and the value at the very right is one.

![Untitled](Images/ProbabilityandStatistics/Untitled%2098.png)

- For continuous distributions, You go over the PDF from the beginning until the end and you simply add up the pieces to create the CDF curve.
- In the graph below, This is a particular case where the curve starts and ends at finite points.
- The blue curve in the left, the density function could go infinitely to the left or infinitely to the right, in which case, so does the other curve, and that means that it has a **limit of zero** to the **left** and a **limit of one** to the **right**.

![Untitled](Images/ProbabilityandStatistics/Untitled%2099.png)

- These two curves give us the same information except written different. On the **left**, we have to calculate **areas** to find probabilities, and on the **right**, we simply have to look at **heights**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20100.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20101.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20102.png)

## Uniform Distribution

- **Uniform distributions** are probability distributions with equally likely outcomes.

![Untitled](Images/ProbabilityandStatistics/Untitled%20103.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20104.png)

- If you look at the histogram, it seems that one of the bars is slightly taller than the rest however, this is just based on measurements so some deviation is expected.
- Since, any value between 0 and 15 have the same frequency, The pdf must be constant, meaning the height of every interval must be equal, and the height is equal 1 / 15 = 0.06
- In a **discrete uniform distribution**, outcomes are discrete and have the same probability.

![Untitled](Images/ProbabilityandStatistics/Untitled%20105.png)

- In a continuous uniform distribution, outcomes are continuous and infinite.

![Untitled](Images/ProbabilityandStatistics/Untitled%20106.png)

- When interval **a, b** is big the **height** of the **PDF** is **small**, and the **height** it quickly increases as the **interval** gets **short**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20107.png)

- For the general uniform distribution between a and b, you can see that
- The **CDF** is the ratio $\frac {x-a} {b-a}$ for $a \leq x < b$
- If **x** $\leq$  **a**, you have **0** cumulative probability.
- For **x** bigger than **b**, you've gathered all the probability, So the CDF is **1** after that point.

![Untitled](Images/ProbabilityandStatistics/Untitled%20108.png)

## Normal Distribution

- The **normal distribution**, also named **Gaussian distribution.**
- If we take a **binomial distribution** with a large number of **n** and plot it, it will look like a **bell curve**, That **bell curve**  is called the **normal distribution.**
- This means that when **n** is very **large**, the **binomial distribution** can be approximated by a **Gaussian distribution.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20109.png)

- If we have a binomial distribution data that looks like a bell curve, and we want to fit a normal distribution to it:
    - First, We're going to try to fit it with this curve.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20110.png)
    
    - Then, look at where the blue data is centered, That’s the mean $\mu$. By subtracting the mean of the blue data from the orange curve, it will be centered.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20111.png)
    
    - Then, We need to widen the orange curve to fit the blue data by dividing the orange curve with the blue data **standard deviation $\sigma$ ,** The standard deviation tells us how thick the curve is.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20112.png)
    
    - Then, We need to adjust the height of the orange curve, we need to divide by the area of the orange curve to better fit the blue curve.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20113.png)
    
    - Now it fits very well, That is the formula for gaussian or normal distribution.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20114.png)
    
- **Normal Distribution** formula:

![Untitled](Images/ProbabilityandStatistics/Untitled%20115.png)

- The **normal distribution probability density function** is always positive, although it's very small for big and for negative big numbers.

![Untitled](Images/ProbabilityandStatistics/Untitled%20116.png)

- If you have a random variable **X** with a **probability density function**, you can write it like this:
    - $\mathcal{N}$ stands for **normal** and $\sigma^2$ is the **variance.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20117.png)

- How to **standardize** any **normal distribution?**

![Untitled](Images/ProbabilityandStatistics/Untitled%20118.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20119.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20120.png)

## Chi-Squared Distribution $x^2$

- Let’s say You send the message 10010 through communication channel. The channel has noise which will affect the message you send.
- you receive the message 10010 plus some **noise** that affected the signal, and let's call the noise **Z,** which has a **random** nature.

![Untitled](Images/ProbabilityandStatistics/Untitled%20121.png)

- One common assumption in communications, and the **noise Z** has a **Gaussian distribution** with **mean** equals **0**
- One measure that is very useful in communications is the **noise power**, which is roughly modeled by the **square** of the **noise**.
- This measure is important because it is associated with the **variance** or **dispersion** of the **noise** and will determine how hard it is to correctly **interpret** the received signal.

![Untitled](Images/ProbabilityandStatistics/Untitled%20122.png)

- In order to get the distribution of **W,** we're going to assume that **Z** follows a **standard normal distribution** with parameter **0** and **1.**
- Each value of **W** can be achieved with two different values of **Z**, which are $- \sqrt[2] {w}$ and $\sqrt [2] {w}$.
- The probability that W $\leq w$ is the **area** under the **PDF curve** on the Gaussian between these two numbers.
- You can get the **CDF** for **W** by finding these **areas** for each possible value **W**.
- Notice that for small values of **W**, you gain area at a much quicker rate. This is because the **Gaussian distribution** concentrates **probability** around **0**.
- This is known as the **Chi-squared distribution** with **one** **degree** of **freedom**

![Untitled](Images/ProbabilityandStatistics/Untitled%20123.png)

- Since the **CDF** is the **integral** of the **PDF**, Then you can easily find the **PDF** by taking the **derivative** of the **CDF**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20124.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20125.png)

## Sampling from a Distribution

- let’s say you have a data set, for example, and you need it to be larger, but you can't go and collect more data, it's too expensive.
- So what can we do? we can create some **synthetic** data that looks a lot like the original one.
- A way to do this is to **construct a distribution** out of this data, and then **sample** out of it. And by **sampling** I mean picking points that have the probabilities given by the original distribution.
- As you know, the **probabilities** of the **outcomes** add to one, So you can **stack** them up together, Then do the following steps to pick random colors with the probabilities given in this graph:

![Untitled](Images/ProbabilityandStatistics/Untitled%20126.png)

- Another way to solve this problem, Create another plot here where we simply rotate at the bars, Push them to the right and draw that **red line**, which is actually the **cumulative distribution function**.
- Now all you have to do is you **sample uniformly** from the **vertical interval,** Then read out what the value is in the horizontal **axis** of the **cumulative distribution function.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20127.png)

- We can also do this for a continuous distribution, Let's say, the **Gaussian distribution** on the left.
- Picking randomly from the distribution is not easy because calculating areas is hard.
- But if we take a look at the **CDF** on the right and then pick **uniformly** from that gray interval, the interval from 0 to 1, that's vertical.
- Then take a look at where this hits the distribution and where it hits the **horizontal axis**.
- So this points over here are actually distributed based on the **normal distribution** on the left.
- So the **CDF**, both for the **discrete** and the **continuous case**, is a very useful method when you want to **sample** from a particular distribution.

![Untitled](Images/ProbabilityandStatistics/Untitled%20128.png)

# Describing Distributions

## Expected Value

- The **mean** has another more formal name for it used in probability called **expected value.**
- The equation below written this way is the **weighted average** of the values of the variable where the weights are the probabilities of each possible value.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/d250b10f-6ab9-416a-af02-9d3f6db49800/Untitled.png)

- In general, if you have a discrete random variable **X**, it will have a probability mass function that provides the probability of each possible **X** can take.
- The **expected value** will be **X** times the probability of **X** summed over all possible values of X.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/07f29093-b656-439c-a1df-8223963c9651/Untitled.png)

- Since we're performing a **weighted average** of the values the variable can take, if every value on your data has the same probability or mass, then the **expected value** will be right in the middle.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/561fa75e-7943-49de-9e97-05562a7cfaef/Untitled.png)

- In both cases, you are summing up all the possible values of X, but in the **discrete case** you weight the sum using the **probability mass function**, and in the **continuous case** you are using the **probability density function**.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/acd749c5-fc95-4013-b9d7-bcde1a6552e8/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/43927810-769f-4fd8-884b-a28aa6d3afb4/Untitled.png)

- There's a common misconception, The mean doesn’t generally split the data in half

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/d3d49cb4-85a9-40f0-bb22-a7e5986209aa/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/c73e9fa1-1afc-41ca-bdef-acab3e02d4d4/Untitled.png)

## Other Measures of Central Tendency: Median and Mode

- The **mean** is not the only way to measure the center of a distribution, There are other ways to measure what are the central or typical values for a probability distribution.
- the **average** here is a little **deceiving.**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/1b91dc0f-8935-468c-8872-e85f3f4e8d24/Untitled.png)

- Michael Jordan’s salary tips the scale So he brings the average up for everybody and the average is no longer measuring what the average student makes. It got skewed by that one point on the right.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/2f486667-9c61-4385-a43f-55a9b71c9030/Untitled.png)

- To fix this problem we can consider a different metric. Put all salaries in order. So now it's just the position that matters then we can pick the number in the very middle. so the salary in the very **middle** is the **median.**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/cae94a1b-af6e-4724-9b4c-5c3a9a61b291/Untitled.png)

- If you have an even number of points, then you simply take the average of the two points in the middle.
- So the moral here is when the **average** is **deceiving**, go for the **median** and you may get a better idea of your data.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/3e340703-649d-4214-a4aa-9bda6af3ddbf/Untitled.png)

- Another way to describe the center of a distribution is called the **mode**.
- Mode is  the value at which the **distribution** has the **highest probability**, which makes it the most frequent one.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/adadc0cf-3f5a-4f3b-8c7e-0d6e4f5d6085/Untitled.png)

- The mode in a **discrete distribution** is the point where the tower is the **highest** and for a **continuous distribution** the **mode** is simply the value that has the **highest height**.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/9288e505-0c0d-4cff-9b47-fb09d789b7fc/Untitled.png)

- The **mode** may not be unique as in a **distribution** where the **maximum** value gets repeated many times and that's called a **multimodal distribution**. And in places like the **uniform distribution**, for example, everything is a **mode**.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/1e10794d-d86b-41bb-8169-727e9aa27d7a/Untitled.png)

## Expected Value of a Function

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/ed095397-dcac-415d-8e0f-7bf55ea170a7/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/f5a7b145-c8dd-4334-a3bb-db3d233293a6/Untitled.png)

## Sum of Expectations

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/0cf18700-7a5d-4190-aed6-6d80c22cb870/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/362a1eda-25e3-429c-924a-5228b2d386a7/Untitled.png)

## Variance

- It turns out that expected value doesn't tell us the whole story about the distribution.
- For example, two distributions may have the same expected value, but one of them can be very narrow and the other one can be very wide. This is captured by something called the **variance**

![Untitled](Images/ProbabilityandStatistics/Untitled%20129.png)

- One way to think of the idea of spread is how far away the points are from the expected outcome.
- If the spread is small, you would expect most points to be close to the expected value. With a bigger spread, you would expect most of the points to be farther away.

![Untitled](Images/ProbabilityandStatistics/Untitled%20130.png)

- The preferred method of measuring **spread** is to actually **square** the **deviations**.
- Using **squared deviation** All of the values will now be **positive**, which is what the **absolute deviation** attempted to do, but without introducing some of the mathematical issues of the **absolute value function**.
- Then you can calculate the **variance** using the **expected squared deviation.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20131.png)

- The **Variance** Formula contains four steps:
- Notice, This is just like **expected value**, that this is a **weighted average**. In these examples, all the outcomes had equal probability. But if they didn't, you would use the probability of each outcome as weights.

![Untitled](Images/ProbabilityandStatistics/Untitled%20132.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20133.png)

- **Variance** has an alternative formula:

![Untitled](Images/ProbabilityandStatistics/Untitled%20134.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20135.png)

- **Adding** a number to a random variable just changes the point the new distribution is centered around, but it doesn't impact the spread.
- **Multiplying** x by a value, however, will impact the spread of your data.

![Untitled](Images/ProbabilityandStatistics/Untitled%20136.png)

## Standard Deviation

- The **variance** is a pretty useful way to measure the **spread of a distribution**. However, it has one small drawback, the **units.**
- We can take the **square root of the variance**, we call that the **standard deviation**.
- **Standard deviation** is a pretty useful way to measure the **spread of a distribution** using the same **units** of the **distribution**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20137.png)

- In the **normal distribution**, the **standard deviation** is very useful.
- A good visual cue to determine the value of **sigma** is where the bell starts changing concavity.
- In statistics, it's very common to talk about being within 1 or 2 or 3 standard deviations of the mean when you're talking about a normal distribution.

![Untitled](Images/ProbabilityandStatistics/Untitled%20138.png)

## Sum of Gaussians

- How to add two **gaussian distributions**?
    - Suppose you can model the **processing time** with a Gaussian distribution with mean 10 and standard deviation 2 and The total latency as Gaussian with mean 5 and standard deviation 1, and suppose also that these two variables are independent of each other.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20139.png)
    
    - Get the corresponding densities for the processing time and for the latency.
    - Then sample each variable 10,000 times and draw the resulting histograms. They both fit the curves pretty well.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20140.png)
    
    - With these samples, you can now generate 10,000 samples off the response time.
    - Notice that R is still **Gaussian**. We need to calculate the parameters of this **Gaussian.**
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20141.png)
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20142.png)
    
    - Now in general, if you have a linear combination of variables where X and Y are both **Gaussian** and both of them are **independent**, then the resulting variable follows a **Gaussian distribution**.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20143.png)
    

## Standardizing a Distribution

- When you have a distribution with **mean** $\mu$, it's always nicer when the distribution has **mean 0**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20144.png)

- Why is it that the new variable $x - \mu$ has **expectation 0**?

![Untitled](Images/ProbabilityandStatistics/Untitled%20145.png)

- In the same way that we want the **mean** to be **0** we want the **standard deviation** to be **1**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20146.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20147.png)

- **Standardizing** a **distribution** has several benefits:
    - Firstly, it transforms datasets into a standard scale, making it easier to compare between different datasets.
    - Secondly, it simplifies statistical analysis, particularly when using techniques that assume a standard normal distribution.
    - Finally, standardizing features in machine learning can improve the convergence rate of optimization algorithms and prevent some features from dominating others, leading to improved model performance.

![Untitled](Images/ProbabilityandStatistics/Untitled%20148.png)

## Skewness and Kurtosis: Moments of a Distribution

- The **expected value** and the **variance** or **standard deviation** paint a really good picture of the distribution.
- However, there are a lot of subtleties that are not captured by them. Some of them are called **skewness** and **kurtosis.**
- Before we get into other ways to measure distribution, We need to know about the **moments**.
- $E[X]$ is the first moment,  $E[X^2]$ is the second moment, etc..

![Untitled](Images/ProbabilityandStatistics/Untitled%20149.png)

## Skewness and Kurtosis - Skewness

- If you look at these two games, they are the exact opposite.

![Untitled](Images/ProbabilityandStatistics/Untitled%20150.png)

- As you can see, these two games are tremendously different.
- Let's see if that difference can be detected by either the **expected value** or the **variance**.
- The **expected value** of these two games is the same, Also the **variance** of these two games is the same but they're vastly different games.

![Untitled](Images/ProbabilityandStatistics/Untitled%20151.png)

- seems like these two distributions have the same first moment and the same second moment.

![Untitled](Images/ProbabilityandStatistics/Untitled%20152.png)

- In order to tell them apart, We're going to use the **third moment**, the **expected value** of the **cube of the variable**.
- The **cube** of the **variable** detects if you have numbers that are **skewed** towards the **right** or **skewed** towards the **left**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20153.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20154.png)

- **Skewness** is the **expected value** of the **centered** and **standardized distribution cubed**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20155.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20156.png)

## Skewness and Kurtosis - Kurtosis

- which game is riskier?
- The game 2 seems a lot more conservative with the $0.10. But once you throw in the $10, it becomes a little riskier, but the probability that you win $10 is very small, or that you lose $10.

![Untitled](Images/ProbabilityandStatistics/Untitled%20157.png)

- These two games have the exact same **variance**. Neither one of them is riskier than the other one.
- They also have the same **expected value** and **skewness**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20158.png)

- So in order to describe to someone that these 2 games are different, We should go for the **fourth moment.**
- It turns out when the **distribution** has very large numbers very far away from the center, even if their probabilities are tiny, $E[X^4]$ captures this.

![Untitled](Images/ProbabilityandStatistics/Untitled%20159.png)

- **kurtosis** is the **expected value** of $x- \mu$ divided by $\sigma$  raised to the 4 power.

![Untitled](Images/ProbabilityandStatistics/Untitled%20160.png)

- what does kurtosis say of a distribution?
    - If the tails are very **skinny**, the **kurtosis** is **small**.
    - If the tails are **thicker**, then we have a **large kurtosis**.
- Even if they have the same variance, the variance may still not be able to pick up. Because even if you have thick tails, you may have a very skinny distribution in the middle. But kurtosis is a much more sensitive measure for the thickness of the tails of the distribution.

![Untitled](Images/ProbabilityandStatistics/Untitled%20161.png)

- We’ve learned many ways to tell distributions apart **expected value**, **Variance, standard deviation**, **skewness**, and **kurtosis**. With these four tools, you'll be able to say a lot about your **probability distributions**.

## Quantiles and Box-plots

![Untitled](Images/ProbabilityandStatistics/Untitled%20162.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20163.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20164.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20165.png)

## Visualizing Data: Box-plots

- There's a visualization called the **box plot** or the **box and whiskers plot**, which is a standardized way of displaying the distribution of your data based on five statistics the **minimum**, the **maximum**, the **median**, the **first quartile**, and the **third quartile**.
- The **interquartile range** $Q3- Q1$ is  where 50% of your data lies.

![Untitled](Images/ProbabilityandStatistics/Untitled%20166.png)

- Note that to draw the **whiskers**. You draw two lines going from the end of the box until       $Q1 - 1.5  * IQR$ and one going from $Q3 + 1.5 * IQR$.
- The **whiskers** should never go beyond the **max** and the **minimum** values of the data set. So if that happens, like in this example, simply cut the whisker at those values.

![Untitled](Images/ProbabilityandStatistics/Untitled%20167.png)

- The **box plot** is very useful, You can get plenty of insight on your data.
- In this case, you can easily see that the data is skewed because $Q3-Q2$  is way bigger than $Q2-Q1$.
- For this small case, we can see that there are no extreme points or **outliers** since both **whiskers** end at the **max** and min values. If there are any values that are **outside** of the **whiskers**, then we say that those are **outliers**, they're too far from the center.
- the length of the **box** and the **whiskers** allow you to analyze the **dispersion** of the data.

![Untitled](Images/ProbabilityandStatistics/Untitled%20168.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20169.png)

## Visualizing Data: Kernel Density Estimation

- All these samples seem to come from a continuous random variable. You know that you can describe your **data distribution** for **continuous** variables with the **probability density(PDF**).

![Untitled](Images/ProbabilityandStatistics/Untitled%20170.png)

- Can we get how **PDF** looks like from this data?
    - Plot a **histogram** of the dataset because a **histogram** satisfies all the conditions of a density function.
        - It's **positive** and the **area under the curve** is **one**.
        - However, It's not a great approximation because:
            - **PDFs** are usually a **smooth** function
            - Also the **discontinuities** of the bars appear from the technique used to compute histograms and not from the data itself.
            - In other words, the actual distribution where these come from may have a pretty **smooth density function**, but since we're taking a **histogram**, it looks like it has a lot of peaks.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20171.png)
    
    - To approximate the **PDF** of our data from a **histogram** we can use a **kernel density estimation**
    - We start by making our **observations** in a graph. Then you want each point of your dataset to have an effect that spreads around the observation point because we want high density where there's a lot of points and low density where there's no points.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20172.png)
    
    - Draw a **Gaussian curve** on top of each data point. This is called the **kernel**. You could choose other functions than the **Gaussian**.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20173.png)
    
    - The value of **Sigma** you choose for the Gaussian density will determine how far the effect of each point spreads. It could be thin or it could be wide.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20174.png)
    
    - Lastly, all you have to do is **multiply** everything by $1/n$ and **sum** all the blue curves to get **area under the curve** equal one.
    - It Doesn't look so great because you've try to approximate a density from just 12 data points.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20175.png)
    
    - If you try from many points, then you're actually going to get a really nice smooth function that actually resembles the PDF well. This is a way to approximate the PDF based on the data.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20176.png)
    

## Visualizing Data: Violin Plots

- **Violin plots** contains the information of both the **kernel density estimation** and the **box plots**
- All you do is you put the **kernel density estimation** at the side of the **box plot**.
- The **violin plot** looks like you have the **KDE** curve, the **mean,** the **quartiles** and the **whiskers.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20177.png)

## Visualizing Data: QQ Plots

![Untitled](Images/ProbabilityandStatistics/Untitled%20178.png)

- Sometimes a quick inspection of something as simple as a bcan let us know that data is in **Gaussian**.
- For example, the left one doesn't look very **Gaussian**, and the right one looks a little **gaussian**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20179.png)

- To get a more exact answer if the data is **Gaussian** or not, We use **quantile quantile plots     (QQ plots)**.
- This present a graphical comparison between **quantiles** in your data and **quantiles** from a **normal distribution**.
- If your data was actually **normally distributed**, then the points should be close to this **orange** line.
- This **QQ plot corresponds** to a data that’s **not Gaussian**. This is especially obvious if you look at the two marked areas where the scatter plot really splits from the orange line.
- Also, There's more concentration of points on the area to the left than the one to the right is a sign that the data is skewed.

![Untitled](Images/ProbabilityandStatistics/Untitled%20180.png)

- This **histogram** looks pretty **bell shaped**, and the **QQ plot** with this data confirms it. This time the quantiles are pretty much aligned around the orange line, which suggests a **Gaussian distribution** for the data.

![Untitled](Images/ProbabilityandStatistics/Untitled%20181.png)

# Probability Distributions with Multiple Variables

## Joint Distribution(Discrete)

- We’ve learned the **probability distributions** for **one variable**, But what if we want to look at **two variables,** For that we need a **joint distribution.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20182.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20183.png)

- We can calculate these probabilities easier if we organize the data properly.
- We can turn the counts into a **probability mass function** when we divide everything by number of observations. to get the probabilities for each possible combination of data.

![Untitled](Images/ProbabilityandStatistics/Untitled%20184.png)

- All possible combinations of x and y are what's called a **joint distribution**.
- In this case, it's a discrete **joint distribution** because both ages and heights are **discrete variables**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20185.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20186.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20187.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20188.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20189.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20190.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20191.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20192.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20193.png)

## Joint Distribution(Continuous)

- If the variables are **continuous,** It's called a **joint distribution of continuous variable.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20194.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20195.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20196.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20197.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20198.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20199.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20200.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20201.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20202.png)

## Marginal and Conditional Distribution

- Say that I have a distribution of ages and heights, but I don't care about age anymore, I just want the distribution of heights. What I have to do is aggregate over all the ages. That’s called a **marginal distribution**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20203.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20204.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20205.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20206.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20207.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20208.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20209.png)

## Conditional Distribution

- Another thing I can do is take slices. For example, I have the full distribution of age and height. And you want the height distribution of people of age 30, So what you do is take a slice and that is called the **conditional distribution**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20210.png)

- If you fix X=9, then you are only looking at this row. Of the whole table, That is my probability distribution of height for kids of age 9.
- If I want to find for example, P(y) given x=9 of 49, then that's going be this value over here.
- However, there's a small caveat. Notice that in a probability distribution, everything has to add to one. The numbers in this row do not add to one because they add to 4/10.
- What you can do is to **normalize,** **Divide** everything by the **row sum.**
- Notice that **normalizing** and **dividing** by the **row sum** is actually applying the **conditional probability rule**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20211.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20212.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20213.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20214.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20215.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20216.png)

## Covariance of a Dataset

- **Covariance** captures the relation between two variables.

![Untitled](Images/ProbabilityandStatistics/Untitled%20217.png)

- How to calculate **covariance?**
    - First, We will center the graphs by subtracting the **mean** of X from the x coordinates and the **mean** of Y for the y coordinates in order to get this **center** point to be at the **origin**.
    - Then divide by the **standard deviation** of x and of y in order to have this really nice plots where the X **variance** and the Y **variance** are both one.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20218.png)
    
    - The plot in the left, If you move to the right, the coordinates move up.
    - The one in the middle, there seems to be no rule.
    - The one on the right, if you move to the right, the coordinates move down.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20219.png)
    
    - The **left graph**, The majority of the coordinates of x and the coordinates of y have the **same sign**.
    - The **right graph**. The majority of the coordinates of x and the coordinates of y have **opposite** sign.
    - The one in the **middle**, nothing seems to happen. It seems like a land with no rules.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20220.png)
    
    - Notice What happens when take the sum of the product of coordinate X and Y.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20221.png)
    
    - This is the formula for **covariance,** It is the sum of the products x, y but, You first have to **center** the data and then take the **average** of all these products.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20222.png)
    
    - The **Covariance** will tell us if one variable has influence on the other one.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20223.png)
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20224.png)
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20225.png)
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20226.png)
    

## Covariance of a Probability Distribution

![Untitled](Images/ProbabilityandStatistics/Untitled%20227.png)

- If you look at each player at a time, by taking the **expected value** of each players.
- It turns out all three games have the same **expected value**, So these games are basically the same if you only think of one player at a time.
- It turns out that these games are also similar in terms of **variance**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20228.png)

- So in order to get the difference we have to look at both players at the same time, So we need to look at the **covariance.**
- The **Covariance** of the first game is 1, Means it is positive shows this correlation show that if player one wins, player two also wins.

![Untitled](Images/ProbabilityandStatistics/Untitled%20229.png)

- Game 2 is the complete opposite. This one has a covariance of -1, which show that if player one wins, player two loses and vice versa.

![Untitled](Images/ProbabilityandStatistics/Untitled%20230.png)

- Game 3 has a covariance of 0, which means we can’t infer the relation between the two players.

![Untitled](Images/ProbabilityandStatistics/Untitled%20231.png)

- What about a game 4 with 3 players and **unequal probabilities**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20232.png)

- Since we have unequal probabilities, We need to modify the covariance formula.

![Untitled](Images/ProbabilityandStatistics/Untitled%20233.png)

- We get a positive **covariance** because they win and lose together.

![Untitled](Images/ProbabilityandStatistics/Untitled%20234.png)

- The **Covariance** of a **joint continuous distribution.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20235.png)

## Covariance Matrix

![Untitled](Images/ProbabilityandStatistics/Untitled%20236.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20237.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20238.png)

## Correlation Coefficient

- Can we tell the **correlation** is stronger using the **covariance**? No, because it could be that the numbers in the right are just much bigger numbers.

![Untitled](Images/ProbabilityandStatistics/Untitled%20239.png)

- Here comes the **correlation coefficient**, It is a number that lies between -1 and 1.
- **-1** is when you have two variables that are completely **negatively correlated**.
- **1** is when you have two variables that are completely **positively correlated.**
- **0** is when they're completely **independent**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20240.png)

- The **correlation coefficient** is simply the **covariance** except divided by **sigma x sigma y**, so it's a **standardized covariance**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20241.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20242.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20243.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20244.png)

## Multivariate Gaussian Distribution

- The **Gaussian distribution** in more variables is called the **multivariate Gaussian.**
- If you have two variables and look at their marginals and notice they are both **gaussian distributions.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20245.png)

- If the two variables were **independent**, then the **joint PDF** will be the **product** of the **marginal PDFs.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20246.png)

- Both distributions are **bell-shaped**, but in the **independent** case, the distribution is completely **symmetric**. In the **dependent** case, you see that distribution is **elongated** along a line with positive slope.

![Untitled](Images/ProbabilityandStatistics/Untitled%20247.png)

- The **dependent distribution** on the right has some **deformation** because it turns out there is a **positive correlation** between height and weight
- On the left, you would have two **independent** variables, and thus you have those circles.

![Untitled](Images/ProbabilityandStatistics/Untitled%20248.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20249.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20250.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20251.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20252.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20253.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20254.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20255.png)

# Population and Sample

## Population and Sample

- The **sample size** could be anything smaller than the **population size**, The idea is to pick a number that is small enough to be manageable, but also large enough to significant.

![Untitled](Images/ProbabilityandStatistics/Untitled%20256.png)

- When taking a **sample** from a **population**, **Random Sampling** is better in estimating the **population** properties.

![Untitled](Images/ProbabilityandStatistics/Untitled%20257.png)

- If you want to run the experiment again, so you take another four people. Well, This is not good Because you took the first sample set at random. But the second one depends on the first one because you couldn't repeat the people.
- The second sample isn’t good Because it is **dependent** on the first one.

![Untitled](Images/ProbabilityandStatistics/Untitled%20258.png)

- You have to start all over every time. People are allowed to be **repeated** and that is very important.

![Untitled](Images/ProbabilityandStatistics/Untitled%20259.png)

- You have to make sure is that the **samples** are **identically distributed**. Whatever rule you use to pick the first one needs to be the same rule you use to pick the second one.

![Untitled](Images/ProbabilityandStatistics/Untitled%20260.png)

- In machine learning and data science, we often use **samples** to train models and make predictions because we can't look at the entire universe of data.
- It is important to have a **representative** dataset. Having a **representative dataset** means the **distribution** of your dataset is the same as the one for the population.

![Untitled](Images/ProbabilityandStatistics/Untitled%20261.png)

## Sample Mean

- The **population mean** is called $\mu$ .

![Untitled](Images/ProbabilityandStatistics/Untitled%20262.png)

- The **sample mean** is $\bar x$.

![Untitled](Images/ProbabilityandStatistics/Untitled%20263.png)

- We have two sample size of 6 below, $\bar x_1$ has a better estimate of the **population mean**, due to the fact that we did more **random sampling**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20264.png)

- It’s good to know that the **bigger** the **sample size**, the better the estimate you're going to get from your **sample** of the **population mean.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20265.png)

## Sample Proportion

![Untitled](Images/ProbabilityandStatistics/Untitled%20266.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20267.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20268.png)

## Sample Variance

- **Variance** is a measure of how spread out your data is. It is related to how far points are from their **mean**.
- In statistics you usually won't have access to the entire **population**. You'll only have a **sample**. you won't have $\mu$ or N.

![Untitled](Images/ProbabilityandStatistics/Untitled%20269.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20270.png)

- Using **sample mean** in **sample variance** equation introduced some errors that make this equation a little bit **biased.**
- **Bias** is a term in statistics that just means that the formula here will either over or underestimate the value it's targeting.
- In this case, This equation would slightly **underestimate** the true value of the **population variance**.
- This doesn't mean that our first estimation is wrong, but maybe we can improve this undershooting of **variance**.
- To solve the **bias** problem introduced from using the **sample mean** instead of the **population mean**, We will adjust the **variance formula**, By subtracting one from **sample number n.** we will call this new way to estimate **variance** $s^2$

![Untitled](Images/ProbabilityandStatistics/Untitled%20271.png)

- Notice that if using **n or n-1** makes a **significant impact** on your estimated variance, You might have bigger problems than deciding whether to divide by **n or n-1** because it probably means you have a **small sample size** and should be wary of making strong conclusions.
- Also Some accepted statistical techniques use **n** in the **denominator** to estimate **variance**. For example, **maximum likelihood estimation.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20272.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20273.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20274.png)

## Law of Large Numbers

![Untitled](Images/ProbabilityandStatistics/Untitled%20275.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20276.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20277.png)

## Central Limit Theorem - Discrete Random Variable

## Central Limit Theorem - Continuous Random Variable

# Point Estimation

## Point Estimation

## Maximum Likelihood Estimation: Motivation

## MLE: Bernoulli Example

## MLE: Gaussian Example

## MLE: Linear Regression

## Regularization

## Back to Bayesians

## Bayesian Statistics - Frequentist vs. Bayesian

## Bayesian Statistics - MAP

## Bayesian Statistics - Updating Priors

## Bayesian Statistics - Full Worked Example

## Relationship Between MAP, MLE and Regularization