# Probability and Statistics

# Introduction to Probability

## What is Probability?

- **Probability** is a measure of how likely an event is to occur.
- We can calculate the **probability** by dividing the number of favorable outcomes by the total number of possible outcomes.

![Untitled](Images/ProbabilityandStatistics/Untitled%200.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%201.png)

- In probability, an **experiment** is any process that produces an outcome that is uncertain.

![Untitled](Images/ProbabilityandStatistics/Untitled%202.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%203.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%204.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%205.png)

## Complement of Probability

- The **complement** of an event is the probability that the event does not occur.

![Untitled](Images/ProbabilityandStatistics/Untitled%206.png)

- The **complement** rule states that the probability of an event A not occurring is equal to 1 minus the probability of A occurring.

![Untitled](Images/ProbabilityandStatistics/Untitled%207.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%208.png)

## Sum of Probabilities(Disjoint Functions)

- Calculating the sum of probabilities, Considering the Events are **disjoint.**
- **Disjoint events** are events that never occur at the same time. These are also known as **mutually exclusive** events.
- **Disjoint events** are typically represented by the **Union**$(\bigcup)$ .

![Untitled](Images/ProbabilityandStatistics/Untitled%209.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2010.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2011.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2012.png)

## Sum of Probabilities(Joint Functions)

- **Joint events** are events that occur simultaneously or together. In other words, they are events that have outcomes in common.
- **Joint events** are typically represented by the **intersection $(\bigcap)$.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2013.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2014.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2015.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2016.png)

## Independence

- **Independence** happens when the occurrence of one event does not affect the probability of the occurrence of another event.
- When we want the intersection of two events and those events are independent of each other, then it's the **product** of the two **probabilities.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2017.png)

- The **product rule** says probability of A **intersection** B is probability of A times the probability of B, Given that the probabilities are **independent**.

![Untitled](Images/ProbabilityandStatistics/Untitled%2018.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2019.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2020.png)

## Birthday Problem

![Untitled](Images/ProbabilityandStatistics/Untitled%2021.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2022.png)

## Conditional Probability

- **Conditional probability** is about calculating the probability of an event happening given that another event has already happened.

![Untitled](Images/ProbabilityandStatistics/Untitled%2023.png)

- Recall that the **product rule** for **independent** events said that the probability of **A intersection B** is **P(A) * P(B)**. But this only happen when **A** and **B** were **independent**.
- If we have events that are **not independent.** The probability of **A intersection B**, Is the probability of **A** times the probability **B given A.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2024.png)

- So This is the **general product rule.**
- Notice that when events are independent, the **probability of B given A** is the same as **probability of B** because **A** doesn't make any difference on the occurrence of **B.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2025.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2026.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2027.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2028.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2029.png)

## Bayes Theorem - Intuition

- **Bayes theorem** is one of the most important theorems in probability and is used all over the place, including in machine learning. It is used for spam recognition, speech detection, and many other things.

![Untitled](Images/ProbabilityandStatistics/Untitled%2030.png)

- Since this disease happens to one out of every 10,000, If you have a population of 1 million, Then 999,900 are healthy and 100 are sick.

![Untitled](Images/ProbabilityandStatistics/Untitled%2031.png)

- If we test every single person and the test is 99% effective:
    - Among the healthy people it made a mistake in 1% of them which is 9,999.(Healthy people diagnosed as sick)
    - The other 99% of healthy people were correctly diagnosed as healthy.
    - For the sick people it correctly diagnosed 99 people as sick, Since the test is 99% effective.
    - Also it misdiagnosed 1 person as healthy while the person is sick.

![Untitled](Images/ProbabilityandStatistics/Untitled%2032.png)

- We want to figure out the probability that you're sick, given that you were diagnosed sick.
- Since most of the people who diagnosed as sick are mostly healthy, Ypu get a probability of less than 1%

![Untitled](Images/ProbabilityandStatistics/Untitled%2033.png)

- So you get diagnosed sick, but the probability that you’re sick is less than 1%.
- What we saw there is an example of **Bayes Theorem.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2034.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2035.png)

## Bayes Theorem - Mathematical Formula

![Untitled](Images/ProbabilityandStatistics/Untitled%2036.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2037.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2038.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2039.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2040.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2041.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2042.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2043.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2044.png)

## Bayes Theorem - Spam Example

![Untitled](Images/ProbabilityandStatistics/Untitled%2045.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2046.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2047.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2048.png)

## Bayes Theorem - Prior and Posterior

- **Prior** is the original probability that you can calculate, not knowing anything else.
- Then something happens, and that's the event.
- The event gives you information about the probability. With that information, you can calculate something called the **posterior**.

![Untitled](Images/ProbabilityandStatistics/Untitled%2049.png)

- The **posterior** is always a better estimation than the prior because we have that event that gave us information.

![Untitled](Images/ProbabilityandStatistics/Untitled%2050.png)

## Bayes Theorem - The Naive Bayes Model

- What if we want to build a classifier with more than one word, more than one event.

![Untitled](Images/ProbabilityandStatistics/Untitled%2051.png)

- To calculate the probability that an email is spam given the fact that it contains lottery and winning using Bayes theorem.
- you will encounter a problem which is you’d be calculating the number of spam emails that contain the two words divided by the total number of spam emails.
- Say instead we want find spam email with 100 words, So using **Bayes theorem** I need to divide the number of emails that contain all 100 words divided by the number of spam emails.
- However, asking for an email to contain 100 words is very hard. Maybe there's no emails like that in our database, and so we get 0 over 0. And that's not a good calculation.

![Untitled](Images/ProbabilityandStatistics/Untitled%2052.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2053.png)

- The solution is instead of calculating the precise probability, We’re going to estimate it using the **Naive assumption.**
- **Naive assumption** says that the appearances of the words lottery and winning are **independent** from each other.
- This is obviously not true, Words are not independent from each other, But if we assume that they're independent, then the math works out much better.
- If you look at probability of lottery and winning given spam, that's the probability of an intersection of two things, And if they happen to be **independent**, then their probability is the **product** of the two things.

![Untitled](Images/ProbabilityandStatistics/Untitled%2054.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2055.png)

- This can happen with **N words**. **N** can be huge, but the probability of an email containing all these words given that is spam can be estimated as the **product** of all these probabilities of the email containing each one of the words given that it's spam. And that is the **naïve Bayes algorithm**.

![Untitled](Images/ProbabilityandStatistics/Untitled%2056.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2057.png)

## Probability in Machine Learning

- It turns out that machine learning is a lot about probabilities. Many times in machine learning what you want to do is, you want to calculate a probability of something given some other factors.

![Untitled](Images/ProbabilityandStatistics/Untitled%2058.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2059.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2060.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2061.png)

# Probability Distributions

## Random Variables

- Random variables can take many values. For example, the temperature is a random variable that can take many values. Another random variable is the number of heads I obtain if I toss a coin 10 times.
- You can think of X as a variable that not always has the same value.

![Untitled](Images/ProbabilityandStatistics/Untitled%2062.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2063.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2064.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2065.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2066.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2067.png)

## Probability Distributions(Discrete)

- Imagine if we have all the possible scenarios of an event that can happen. Put them on this horizontal axis and for each one of them, look at the probability that they happen. This forms a **probability distribution.**
- These are the probabilities that can be visualized by a histogram.

![Untitled](Images/ProbabilityandStatistics/Untitled%2068.png)

- All discrete random variables can be modeled by their **probability mass function**, also abbreviated as **PMF**.
- It contains all the necessary information to understand how the probability distributes among all the possible values of the variable.

![Untitled](Images/ProbabilityandStatistics/Untitled%2069.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2070.png)

## Binomial Distributions

- The **binomial distribution** is an example of a discrete distributions
- Say we toss a coin 10 times. How many heads can I obtain? It could be from 0 all the way up to 10. Each one comes with its own corresponding probability, so we draw the histogram of probabilities, and that's the **binomial distribution**.
- What is the probability to obtain two heads when you flip five coins?
    - For each flip, there's a 1/2 chance to get heads or tails. If you multiply these together, you get 1/32, which is the probability of this specific outcome.
    - But there are more ways to obtain two heads out of five, Also it has the same probability as the above one.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2071.png)
    
    - It turns out that there are actually 10 possibilities to get two heads out of five.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2072.png)
    
    - There are some general way to find the number of possible combinations.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2073.png)
    
    - This is called the **binomial coefficient**, and it counts the number of ways you can order two heads and three tails.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2074.png)
    
    - One common property of binomial coefficient is that $\dbinom n k$ is the same as$\dbinom {n} {n-k}$. The reason for this is because obtaining **k** heads is the same thing as obtaining **n-k** tails.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2075.png)
    
    - Here is a general way to write the **PMF** for the number of heads in **n** coin tosses:
        - We will multiply the probability of seeing **x heads** times probability of seeing **n - x tails,** This is the probability of one particular ordering.
        - To take into account all possible orders which is the **binomial coefficient** $\dbinom n k$
        
        ![Untitled](Images/ProbabilityandStatistics/Untitled%2076.png)
        
        - We write the equation like that:
        - The tilde symbol means that the variable X follows the distribution to the right of the expression.
        
        ![Untitled](Images/ProbabilityandStatistics/Untitled%2077.png)
        
        ![Untitled](Images/ProbabilityandStatistics/Untitled%2078.png)
        
    - If **p** is **0.5** and **n = 5**, then you get the following **PMF**.
    - Notice that since **p** is **1/2**, then the **PMF** is **symmetrical.**
        
        ![Untitled](Images/ProbabilityandStatistics/Untitled%2079.png)
        
    - If we have a biased coin say of **p=0.3,** then you have greater chances of seeing a small number of heads, and this is reflected in the **PMF**. ****

![Untitled](Images/ProbabilityandStatistics/Untitled%2080.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2081.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2082.png)

## Binomial coefficient

- Binomial coefficient is the number of ways to obtain **k** elements out of a set of **n** in an **unordered way**.
- So in order to get the binomial coefficient, First we need to get the ordered set of length **k.**

![Untitled](Images/ProbabilityandStatistics/Untitled%2083.png)

- Since we know many of the combinations actually repeat, how many of them repeat?
    - We had that the number of ways of picking **ordered sets of length k** that picked every set many times.
    - **K!** is simply the number of ways of ordering **k** numbers or **k** different objects.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%2084.png)
    
    - So since we want them **unordered**, then we have to divide by **k!**
- That is the expression for $\dbinom n k$ for the binomial coefficient $\dbinom n k$, the number of ways to obtain **k** elements out of a set of **n** in an **unordered** way.

![Untitled](Images/ProbabilityandStatistics/Untitled%2085.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2086.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2087.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2088.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2089.png)

## Bernoulli Distribution

- **Bernoulli** is a very important distribution in probability and statistics, which has one parameter (p), Which is the probability of **success**.

![Untitled](Images/ProbabilityandStatistics/Untitled%2090.png)

## Probability distributions(Continuous)

- When your events are a list, you have a **discrete distribution**. And when your events are an **interval**, you have a **continuous distribution.**
- So with a continuous problem instead of asking what is the probability at specific time, You should ask what's the probability between a certain window time.

![Untitled](Images/ProbabilityandStatistics/Untitled%2091.png)

- This is how a continuous distribution looks like, It is just a bunch of very, very skinny bars, infinitely many of them, that become just a curve.

![Untitled](Images/ProbabilityandStatistics/Untitled%2092.png)

## Probability Density Function

- **Discrete distributions** have a **probability mass function**, on the other hand, **Continuous distributions** have a **probability density function.**
- In **discrete distributions** probabilities responds to **height**, the more the height the more the probability.
- In **continuous distributions** probabilities respond to areas, the thicker the interval, the more probability it is.

![Untitled](Images/ProbabilityandStatistics/Untitled%2093.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2094.png)

- If we want the probability of calls at exactly 2 minutes, then we'll be talking about the area of this segment over here, that is a line segment, and it has no area. So, that's why we say that that probability is 0.

![Untitled](Images/ProbabilityandStatistics/Untitled%2095.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2096.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%2097.png)

## Cumulative Distribution Function

- In **continuous distribution** you have to calculate areas underneath a curve in order to find the probability that a call is between, let's say, two and three minutes.
- The **cumulative distribution** function  shows the actual probability that the call is between zero and a certain number of minutes.
- In order to get **cumulative distribution**, we need to get the **cumulative probability,** and it's this red curve over here. Notice that it always starts at zero and the value at the very right is one.

![Untitled](Images/ProbabilityandStatistics/Untitled%2098.png)

- For continuous distributions, You go over the PDF from the beginning until the end and you simply add up the pieces to create the CDF curve.
- In the graph below, This is a particular case where the curve starts and ends at finite points.
- The blue curve in the left, the density function could go infinitely to the left or infinitely to the right, in which case, so does the other curve, and that means that it has a **limit of zero** to the **left** and a **limit of one** to the **right**.

![Untitled](Images/ProbabilityandStatistics/Untitled%2099.png)

- These two curves give us the same information except written different. On the **left**, we have to calculate **areas** to find probabilities, and on the **right**, we simply have to look at **heights**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20100.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20101.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20102.png)

## Uniform Distribution

- **Uniform distributions** are probability distributions with equally likely outcomes.

![Untitled](Images/ProbabilityandStatistics/Untitled%20103.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20104.png)

- If you look at the histogram, it seems that one of the bars is slightly taller than the rest however, this is just based on measurements so some deviation is expected.
- Since, any value between 0 and 15 have the same frequency, The pdf must be constant, meaning the height of every interval must be equal, and the height is equal 1 / 15 = 0.06
- In a **discrete uniform distribution**, outcomes are discrete and have the same probability.

![Untitled](Images/ProbabilityandStatistics/Untitled%20105.png)

- In a continuous uniform distribution, outcomes are continuous and infinite.

![Untitled](Images/ProbabilityandStatistics/Untitled%20106.png)

- When interval **a, b** is big the **height** of the **PDF** is **small**, and the **height** it quickly increases as the **interval** gets **short**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20107.png)

- For the general uniform distribution between a and b, you can see that
- The **CDF** is the ratio $\frac {x-a} {b-a}$ for $a \leq x < b$
- If **x** $\leq$  **a**, you have **0** cumulative probability.
- For **x** bigger than **b**, you've gathered all the probability, So the CDF is **1** after that point.

![Untitled](Images/ProbabilityandStatistics/Untitled%20108.png)

## Normal Distribution

- The **normal distribution**, also named **Gaussian distribution.**
- If we take a **binomial distribution** with a large number of **n** and plot it, it will look like a **bell curve**, That **bell curve**  is called the **normal distribution.**
- This means that when **n** is very **large**, the **binomial distribution** can be approximated by a **Gaussian distribution.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20109.png)

- If we have a binomial distribution data that looks like a bell curve, and we want to fit a normal distribution to it:
    - First, We're going to try to fit it with this curve.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20110.png)
    
    - Then, look at where the blue data is centered, That’s the mean $\mu$. By subtracting the mean of the blue data from the orange curve, it will be centered.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20111.png)
    
    - Then, We need to widen the orange curve to fit the blue data by dividing the orange curve with the blue data **standard deviation $\sigma$ ,** The standard deviation tells us how thick the curve is.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20112.png)
    
    - Then, We need to adjust the height of the orange curve, we need to divide by the area of the orange curve to better fit the blue curve.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20113.png)
    
    - Now it fits very well, That is the formula for gaussian or normal distribution.
    
    ![Untitled](Images/ProbabilityandStatistics/Untitled%20114.png)
    
- **Normal Distribution** formula:

![Untitled](Images/ProbabilityandStatistics/Untitled%20115.png)

- The **normal distribution probability density function** is always positive, although it's very small for big and for negative big numbers.

![Untitled](Images/ProbabilityandStatistics/Untitled%20116.png)

- If you have a random variable **X** with a **probability density function**, you can write it like this:
    - $\mathcal{N}$ stands for **normal** and $\sigma^2$ is the **variance.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20117.png)

- How to **standardize** any **normal distribution?**

![Untitled](Images/ProbabilityandStatistics/Untitled%20118.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20119.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20120.png)

## Chi-Squared Distribution $x^2$

- Let’s say You send the message 10010 through communication channel. The channel has noise which will affect the message you send.
- you receive the message 10010 plus some **noise** that affected the signal, and let's call the noise **Z,** which has a **random** nature.

![Untitled](Images/ProbabilityandStatistics/Untitled%20121.png)

- One common assumption in communications, and the **noise Z** has a **Gaussian distribution** with **mean** equals **0**
- One measure that is very useful in communications is the **noise power**, which is roughly modeled by the **square** of the **noise**.
- This measure is important because it is associated with the **variance** or **dispersion** of the **noise** and will determine how hard it is to correctly **interpret** the received signal.

![Untitled](Images/ProbabilityandStatistics/Untitled%20122.png)

- In order to get the distribution of **W,** we're going to assume that **Z** follows a **standard normal distribution** with parameter **0** and **1.**
- Each value of **W** can be achieved with two different values of **Z**, which are $- \sqrt[2] {w}$ and $\sqrt [2] {w}$.
- The probability that W $\leq w$ is the **area** under the **PDF curve** on the Gaussian between these two numbers.
- You can get the **CDF** for **W** by finding these **areas** for each possible value **W**.
- Notice that for small values of **W**, you gain area at a much quicker rate. This is because the **Gaussian distribution** concentrates **probability** around **0**.
- This is known as the **Chi-squared distribution** with **one** **degree** of **freedom**

![Untitled](Images/ProbabilityandStatistics/Untitled%20123.png)

- Since the **CDF** is the **integral** of the **PDF**, Then you can easily find the **PDF** by taking the **derivative** of the **CDF**.

![Untitled](Images/ProbabilityandStatistics/Untitled%20124.png)

![Untitled](Images/ProbabilityandStatistics/Untitled%20125.png)

## Sampling from a Distribution

- let’s say you have a data set, for example, and you need it to be larger, but you can't go and collect more data, it's too expensive.
- So what can we do? we can create some **synthetic** data that looks a lot like the original one.
- A way to do this is to **construct a distribution** out of this data, and then **sample** out of it. And by **sampling** I mean picking points that have the probabilities given by the original distribution.
- As you know, the **probabilities** of the **outcomes** add to one, So you can **stack** them up together, Then do the following steps to pick random colors with the probabilities given in this graph:

![Untitled](Images/ProbabilityandStatistics/Untitled%20126.png)

- Another way to solve this problem, Create another plot here where we simply rotate at the bars, Push them to the right and draw that **red line**, which is actually the **cumulative distribution function**.
- Now all you have to do is you **sample uniformly** from the **vertical interval,** Then read out what the value is in the horizontal **axis** of the **cumulative distribution function.**

![Untitled](Images/ProbabilityandStatistics/Untitled%20127.png)

- We can also do this for a continuous distribution, Let's say, the **Gaussian distribution** on the left.
- Picking randomly from the distribution is not easy because calculating areas is hard.
- But if we take a look at the **CDF** on the right and then pick **uniformly** from that gray interval, the interval from 0 to 1, that's vertical.
- Then take a look at where this hits the distribution and where it hits the **horizontal axis**.
- So this points over here are actually distributed based on the **normal distribution** on the left.
- So the **CDF**, both for the **discrete** and the **continuous case**, is a very useful method when you want to **sample** from a particular distribution.

![Untitled](Images/ProbabilityandStatistics/Untitled%20128.png)

# Describing Distributions

## Expected Value

- The **mean** has another more formal name for it used in probability called **expected value.**
- The equation below written this way is the **weighted average** of the values of the variable where the weights are the probabilities of each possible value.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/d250b10f-6ab9-416a-af02-9d3f6db49800/Untitled.png)

- In general, if you have a discrete random variable **X**, it will have a probability mass function that provides the probability of each possible **X** can take.
- The **expected value** will be **X** times the probability of **X** summed over all possible values of X.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/07f29093-b656-439c-a1df-8223963c9651/Untitled.png)

- Since we're performing a **weighted average** of the values the variable can take, if every value on your data has the same probability or mass, then the **expected value** will be right in the middle.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/561fa75e-7943-49de-9e97-05562a7cfaef/Untitled.png)

- In both cases, you are summing up all the possible values of X, but in the **discrete case** you weight the sum using the **probability mass function**, and in the **continuous case** you are using the **probability density function**.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/acd749c5-fc95-4013-b9d7-bcde1a6552e8/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/43927810-769f-4fd8-884b-a28aa6d3afb4/Untitled.png)

- There's a common misconception, The mean doesn’t generally split the data in half

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/d3d49cb4-85a9-40f0-bb22-a7e5986209aa/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/c73e9fa1-1afc-41ca-bdef-acab3e02d4d4/Untitled.png)

## Other Measures of Central Tendency: Median and Mode

- The **mean** is not the only way to measure the center of a distribution, There are other ways to measure what are the central or typical values for a probability distribution.
- the **average** here is a little **deceiving.**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/1b91dc0f-8935-468c-8872-e85f3f4e8d24/Untitled.png)

- Michael Jordan’s salary tips the scale So he brings the average up for everybody and the average is no longer measuring what the average student makes. It got skewed by that one point on the right.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/2f486667-9c61-4385-a43f-55a9b71c9030/Untitled.png)

- To fix this problem we can consider a different metric. Put all salaries in order. So now it's just the position that matters then we can pick the number in the very middle. so the salary in the very **middle** is the **median.**

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/cae94a1b-af6e-4724-9b4c-5c3a9a61b291/Untitled.png)

- If you have an even number of points, then you simply take the average of the two points in the middle.
- So the moral here is when the **average** is **deceiving**, go for the **median** and you may get a better idea of your data.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/3e340703-649d-4214-a4aa-9bda6af3ddbf/Untitled.png)

- Another way to describe the center of a distribution is called the **mode**.
- Mode is  the value at which the **distribution** has the **highest probability**, which makes it the most frequent one.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/adadc0cf-3f5a-4f3b-8c7e-0d6e4f5d6085/Untitled.png)

- The mode in a **discrete distribution** is the point where the tower is the **highest** and for a **continuous distribution** the **mode** is simply the value that has the **highest height**.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/9288e505-0c0d-4cff-9b47-fb09d789b7fc/Untitled.png)

- The **mode** may not be unique as in a **distribution** where the **maximum** value gets repeated many times and that's called a **multimodal distribution**. And in places like the **uniform distribution**, for example, everything is a **mode**.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/1e10794d-d86b-41bb-8169-727e9aa27d7a/Untitled.png)

## Expected Value of a Function

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/ed095397-dcac-415d-8e0f-7bf55ea170a7/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/f5a7b145-c8dd-4334-a3bb-db3d233293a6/Untitled.png)

## Sum of Expectations

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/0cf18700-7a5d-4190-aed6-6d80c22cb870/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/8922aaf7-be84-4ccc-8d4e-ef3a7243439a/362a1eda-25e3-429c-924a-5228b2d386a7/Untitled.png)

## Variance

## Standard Deviation

## Sum of Gaussians

## Standardizing a Distribution

## Skewness and Kurtosis: Moments of a Distribution

## Skewness and Kurtosis - Skewness

## Skewness and Kurtosis - Kurtosis

## Quantiles and Box-plots

## Visualizing Data: Box-plots

## Visualizing Data: Kernel Density Estimation

## Visualizing Data: Violin Plots

## Visualizing Data: QQ Plots

# Probability Distributions with Multiple Variables

## Joint Distribution(Discrete)

## Joint Distribution(Continuous)

## Marginal and Conditional Distribution

## Conditional Distribution

## Covariance of a Dataset

## Covariance of a Probability Distribution

## Covariance Matrix

## Correlation Coefficient

## Multivariate Gaussian Distribution