# Linear Algebra

# Systems of Equations

## Linear Algebra and Machine Learning

- A common machine learning approach to modeling systems is called Linear Regression

![Untitled](Images/LinearAlgebra/Untitled.png)

- You can make accurate predictions of the output under the assumption that this is a linear relationship

![Untitled](Images/LinearAlgebra/Untitled%201.png)

- You’d find the values of the weight and bias terms that solve all these equations at the same time, From this common machine learning model appears a fundamental  concept of linear algebra called a **system of linear equations**.

![Untitled](Images/LinearAlgebra/Untitled%202.png)

- Linear Algebra is all about manipulating vectors and matrices to do powerful calculations, This math is the backbone of many machine learning models.

![Untitled](Images/LinearAlgebra/Untitled%203.png)

## System of Sentences

- The way sentences combined to give you information is similar to the way equations combined to give you information.

![Untitled](Images/LinearAlgebra/Untitled%204.png)

- When a system is complete is called a **non-singular system**, A **non-singular system** is a system that carries as many pieces of information as sentences, It’s the most informative the system can be.
- A **singular system** is less informative than a non-singular one.

## System of Equations

- Just like with **systems** of **sentences**, **systems** of **linear equations** can also be **singular** or **non-singular** based on how much information they carry.

![Untitled](Images/LinearAlgebra/Untitled%205.png)

![Untitled](Images/LinearAlgebra/Untitled%206.png)

## System of Equations as lines and planes

- Linear equations can easily be visualized as lines in the coordinate plane and systems of linear equations can be represented as arrangements of lines in the plane. This way, you can visualize their solutions and their singularity, or non singularity in a much clearer way.

![Untitled](Images/LinearAlgebra/Untitled%207.png)

- The notion of **slope** and **y-intercept** in a line, The **slope** of a line is a measure of its steepness. Mathematically, slope is calculated as "rise over run”,
- For the line on the **left** is $-1$ as for every unit you move to the right, the line move one unit down.
- For the line on the **right**, the **slope** is $\frac{-1}{2}$  because for every unit you move to the right, line moves half a unit down.

![Untitled](Images/LinearAlgebra/Untitled%208.png)

![Untitled](Images/LinearAlgebra/Untitled%209.png)

- In the same way as a linear equation with two variables correspond to a line in the plane. A linear equation on three variables correspond to a plane in space.
- In the same way as we used to intersect lines to get points as the solutions to the systems equations, you can also intersect planes.

![Untitled](Images/LinearAlgebra/Untitled%2010.png)

![Untitled](Images/LinearAlgebra/Untitled%2011.png)

![Untitled](Images/LinearAlgebra/Untitled%2012.png)

## A Geometric Notion of Singularity

- There is an even simpler way to visualize singularity and non-singularity. It involves slightly simplifying the system. Note that system 2 and 3, the singular ones, are very similar since both consists of parallel lines.
- We can contract system 2 and 3 into one bucket by looking at the constants of the systems of equations, These constants are the numbers in the equations that are not accompanying the variables A or B.

![Untitled](Images/LinearAlgebra/Untitled%2013.png)

- If we turn all these constants for all these three systems into 0 and see what happens to the plots. The new systems always have the 0.00 as a solution, so they must pass by the origin.
- Notice that system 1 is still a pair of intersecting lines, so it still has a unique solution. So it's still complete and non-singular.
- System 2 is still a pair of identical lines, so it still has infinite solutions, so it's redundant and singular.
- System 3 went from a pair of non-intersecting parallel lines to a pair of identical lines. So now it has infinitely many solutions instead of none. It went from contradictory to redundant. However, it stayed singular as before.
- So in conclusion, the constants in the system don't matter when it comes to determine the system is singular or non-singular.
- You can now start considering systems of equations where the constants are always 0, These are much simpler.

![Untitled](Images/LinearAlgebra/Untitled%2014.png)

## **Singular vs non-singular matrices**

- **Matrices** have lots of very important properties and they arise from many different places in math. In our case, They will arise from the **coefficients** in a system's of equations in a very natural way.
- **Matrices** like systems of linear equations can also be singular or non-singular.

![Untitled](Images/LinearAlgebra/Untitled%2015.png)

![Untitled](Images/LinearAlgebra/Untitled%2016.png)

## Linear Dependence and Independence

- A system of equation is singular if the second equation carries the same information as the first one. This is the concept of **linear dependence**.

![Untitled](Images/LinearAlgebra/Untitled%2017.png)

![Untitled](Images/LinearAlgebra/Untitled%2018.png)

![Untitled](Images/LinearAlgebra/Untitled%2019.png)

![Untitled](Images/LinearAlgebra/Untitled%2020.png)

![Untitled](Images/LinearAlgebra/Untitled%2021.png)

![Untitled](Images/LinearAlgebra/Untitled%2022.png)

## The Determinant

- There is a much faster way to tell if a matrix is singular or non-singular, It's called the **determinant** and it is a quick formula that returns a zero if the matrix is singular and a number different from zero if the matrix is non-singular.
- the **determinant** is zero if the matrix is **singular** and non-zero if it's **non-singular.**

![Untitled](Images/LinearAlgebra/Untitled%2023.png)

![Untitled](Images/LinearAlgebra/Untitled%2024.png)

- The determinant for three-by-three matrix is a bit more complicated than that for two-by-two matrix, but it is mostly the same.
- Here are the diagonals in 2 x 2 and 3 x 3 matrices:

![Untitled](Images/LinearAlgebra/Untitled%2025.png)

- For the determinant, you're simply going to add the products of the elements in these diagonals and subtract the products of the elements in these diagonals that go the other way around.

![Untitled](Images/LinearAlgebra/Untitled%2026.png)

![Untitled](Images/LinearAlgebra/Untitled%2027.png)

![Untitled](Images/LinearAlgebra/Untitled%2028.png)

# Solving Systems of Linear Equations: Elimination

## **Solving non-singular system of linear equations**

- we will learn a method that finds the solution to a system of linear equations and can tell you if the system is singular or non-singular.

![Untitled](Images/LinearAlgebra/Untitled%2029.png)

![Untitled](Images/LinearAlgebra/Untitled%2030.png)

![Untitled](Images/LinearAlgebra/Untitled%2031.png)

## Solving singular system of linear equations****

![Untitled](Images/LinearAlgebra/Untitled%2032.png)

![Untitled](Images/LinearAlgebra/Untitled%2033.png)

## **Solving systems of equations with more variables**

![Untitled](Images/LinearAlgebra/Untitled%2034.png)

![Untitled](Images/LinearAlgebra/Untitled%2035.png)

![Untitled](Images/LinearAlgebra/Untitled%2036.png)

## Matrix Row-Reduction

- **Matrix row reduction,** also called the **Gaussian elimination**, consists of applying the exact same manipulations except to the rows of a matrix. In order to turn that matrix into a much more simplified form, from which you can extract lots of useful information.
- From the original matrix, you can get the matrix in the intermediate system by applying some row manipulations. An important feature of this matrix is that it has 1s in the main diagonal and zeros underneath the diagonal and is called **Row Echelon Form.**
- Finally, some more manipulations will get you to the matrix with ones in the diagonal and zeroes everywhere else and is called Reduced **Row Echelon Form**

![Untitled](Images/LinearAlgebra/Untitled%2037.png)

![Untitled](Images/LinearAlgebra/Untitled%2038.png)

![Untitled](Images/LinearAlgebra/Untitled%2039.png)

![Untitled](Images/LinearAlgebra/Untitled%2040.png)

- A matrix in **row echelon form** looks like one of the following:
    - For 3 x 3 matrices:
        - On the main diagonal, we can have bunch of ones followed by perhaps a bunch of zeros.
        - On the main diagonal, we can have all ones.
        - On the main diagonal, we can also have all zeros.
        - Below the diagonal, everything is a zero.
        - To the right of the ones any number is allowed
        - Finally to the right of the zeros, everything must be zero.
    - For 2 x 2 matrices:
        - You can have two 1s in the diagonal
        - You can have one 1s in the diagonal.
        - you can have zero 1s in the diagonal.

![Untitled](Images/LinearAlgebra/Untitled%2041.png)

## **Row operations that preserve singularity**

- The same manipulations that you use to solve systems of linear equations can be used in matrices. These are called **row operations** in a matrix
- A very important property that they have is that they preserve the **singularity** of a matrix. In other words, if you apply them to a **singular matrix**, you get a **singular matrix** and if you apply them to a **non-singular matrix**, you get a **non-singular matrix**.
- The Row Operations that preserve singularity:

![Untitled](Images/LinearAlgebra/Untitled%2042.png)

![Untitled](Images/LinearAlgebra/Untitled%2043.png)

![Untitled](Images/LinearAlgebra/Untitled%2044.png)

# Solving System of Linear Equations: Row Echelon Form and Rank

## The Rank of a Matrix

- **Rank of a matrix**  measures how much information that matrix or its corresponding system linear equations is carrying.
- One great application of ranking machine learning is an **image compression**. The image below uses a lot of storage because every pixel intensity has to be stored as a number.
- We can store this image or perhaps a slightly blurrier version of it using significantly less space. By utilizing rank of a matrix.
- It turns out that pixelated images are matrices and the rank of matrix is related to the amount of space that is needed to store that corresponding image.
- There's a very powerful technique on **singular value decomposition(SVD)**, which can reduce the **rank of a matrix** while changing it as little as possible.
- Notice how the images of rank 15 and 50 are very similar to the original and we take a lot less space to store.

![Untitled](Images/LinearAlgebra/Untitled%2045.png)

- The amount of information a system of sentences carries is defined as the rank of the system.

![Untitled](Images/LinearAlgebra/Untitled%2046.png)

- Rank of the matrix is defined as the rank of the corresponding system of equations.

![Untitled](Images/LinearAlgebra/Untitled%2047.png)

- There's a special relationship between the **rank of a matrix** and its **solution space**. The **solution space** for each matrix is the set of solutions to the system of equations when the **constants** are **zero**.

![Untitled](Images/LinearAlgebra/Untitled%2048.png)

- For the first one the solutions are only a=0 and b=0. So that's a point and the dimensional solution space is 0 because the dimension of a point is 0.
- For the second one, the set of solutions was some line and a line has dimension 1. So the dimension of the solution space was 1.
- For the third one, well every a and b works here because any point a and b is a solution to that system. Therefore the solution space is a plane and it has dimension 2.
- **Rank of matrix** is equal to **number** of **rows** in the matrix minus the dimension of the **solution space**.

![Untitled](Images/LinearAlgebra/Untitled%2049.png)

- A matrix is non-singular if only has full rank, namely if the rank is equal to the number of rows.

![Untitled](Images/LinearAlgebra/Untitled%2050.png)

## The Rank of a Matrix in General

- This is how the Rank works for larger matrices.

![Untitled](Images/LinearAlgebra/Untitled%2051.png)

## Row Echelon Form

- There is a simpler way to calculate the rank using **Row Echelon Form of a matrix.**
- We used the systems of equations to find **Row Echelon Form of a matrix**. We can use the row operations to calculate the **row echelon form.**

![Untitled](Images/LinearAlgebra/Untitled%2052.png)

![Untitled](Images/LinearAlgebra/Untitled%2053.png)

![Untitled](Images/LinearAlgebra/Untitled%2054.png)

- The **rank of a matrix** is the number of **ones** in the diagonal of the **row echelon form**. Also a matrix is **NON-singular** if and only if the **row echelon form** has **only ones** and **no zeros.**

![Untitled](Images/LinearAlgebra/Untitled%2055.png)

## **Row echelon form in general**

![Untitled](Images/LinearAlgebra/Untitled%2056.png)

![Untitled](Images/LinearAlgebra/Untitled%2057.png)

![Untitled](Images/LinearAlgebra/Untitled%2058.png)

![Untitled](Images/LinearAlgebra/Untitled%2059.png)

![Untitled](Images/LinearAlgebra/Untitled%2060.png)

![Untitled](Images/LinearAlgebra/Untitled%2061.png)

## Reduced Row Echelon Form

![Untitled](Images/LinearAlgebra/Untitled%2062.png)

- The way to get from the **row echelon form** matrix to the **reduced row echelon form** is simply to use each 1s in the diagonal to disappear all the non-zero entries above.

![Untitled](Images/LinearAlgebra/Untitled%2063.png)

![Untitled](Images/LinearAlgebra/Untitled%2064.png)

![Untitled](Images/LinearAlgebra/Untitled%2065.png)

![Untitled](Images/LinearAlgebra/Untitled%2066.png)

## **The Gaussian Elimination Algorithm**

- It is just the **elimination** method you studied before but restructured to solve a system of equations and formalize so that it can be followed by hand or implemented in code.
- Recall that when you learn about the singularity of a matrix, you ignore the constant values on the right-hand side of your equations. You treated them as though they were just zero.
- To actually solve a **system of equations**, you will use them. To begin, make a matrix from the coefficient in your system of equations and add another column to the right side of your matrix, which holds the constant values. This is called the **augmented matrix**. The vertical line is used to separate the constants, so you remember they're not part of the variables.
- You can use the augmented matrix to solve your system of equations using the Elimination method.

![Untitled](Images/LinearAlgebra/Untitled%2067.png)

- To complete the elimination method, you'll repeatedly find an element called a **pivot** on the diagonal of the matrix. To start, you'll select the top left cell as your **pivot**. Use row operations to set your **pivot** to **one**. Then use row operations to set all the values below your **pivot** to **zero**. After that repeat the process row by row using row operations to simplify the matrix down to the **reduced row-echelon form**.
- Whatever row operations you perform on the matrix will also be applied to the **column** of **constants** you included to form the **augmented matrix**. They'll eventually help us solve the system of equations

![Untitled](Images/LinearAlgebra/Untitled%2068.png)

- here we reached the **Row Echelon Form,** The diagonal is all ones, and below the diagonal, there's only zeros.

![Untitled](Images/LinearAlgebra/Untitled%2069.png)

- Then we will use the information in constant column to actually solve the **system of equations** through a process called **back substitution**.
- Here's how **back substitution** works, You will start from the bottom row and work your way to the top. You'll use the **pivot** from each row to cancel the values in the cells above it.

![Untitled](Images/LinearAlgebra/Untitled%2070.png)

- Do that until you've got a matrix that has all ones in the diagonal and zeros on all other positions.
- Note that the square part of the **augmented matrix** has only 1s in the diagonal. Such a matrix is called the **identity matrix**. By simplifying the matrix to this form using **Gaussian elimination**, you have solved the original **system** of **equations**.

![Untitled](Images/LinearAlgebra/Untitled%2071.png)

- Will Gaussian elimination work if the matrix is singular?
    - We know that if the matrix is singular, then in reduced **row-echelon form**, you'll have a row that is all zeros. Once you get here, the algorithm just stops.
    - The whole point of **Gaussian elimination** is to find solutions to a **system** of **equations**. If you find a row of zero, you know your matrix is singular and there is no solution.
    
    ![Untitled](Images/LinearAlgebra/Untitled%2072.png)
    
    - You can still determine if your matrix is **contradictory** and has **no solutions** or if it has **infinitely** many **solutions**.
    - To do that, you just need to look at the **column** of **constants**. If the **constant** value in the **row** of **zero** is also **zero**, the row just says 0a+0b+0c = 0. No matter what values you choose for A, B, and C, the left side will always equal zero, and this equation will be true. The system has **infinitely many solutions.**
    
    ![Untitled](Images/LinearAlgebra/Untitled%2073.png)
    
    - if  you get a matrix with the **constant value** in the **third** row changing to say four. Now the last row states that 0a+0b+0c = 4. No matter what values of A, B, and C you choose, the left side of the equation will equal zero, but the right side equals four. This means that the system h**as no possible solutions**.
    
    ![Untitled](Images/LinearAlgebra/Untitled%2074.png)