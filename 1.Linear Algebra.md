# Linear Algebra

# Notations Used in This Course

| A,B,C | capital letters represent matrices |
| --- | --- |
| $u,v,w$ | lowercase letters represent vectors |
| $A$ of size $m×n$ or $(m×n)$ | matrix $A $ has $m $ rows and $n $ columns |
| $A^{T}$ | the transpose of matrix $A$ |
| $v^{T}$ | the transpose of vector $v$ |
| $A^{-1}$ | the inverse of matrix $A$ |
| $det⁡(A)$ | the determinant of matrix A |
| $AB$ | matrix multiplication of matrices $A $and $B$ |
| $u⋅v; ⟨u,v⟩$ | dot product of vectors u and v |
| $R$ | the set of real numbers, e.g. $0,−0.642,2,3.4560,−0.642$ $,2,3.456$ |
| $R^{2}$ | the set of two-dimensional vectors, e.g. $\begin{bmatrix} 2 & 3\end{bmatrix} ^{T}$ |
| $R^n$ | the set of $n$-dimensional vectors |
| $v \in R^2$ | vector $v $ is an element of $R^2$ |
| $∣v∣_1​$ | L1-norm of a vector |
| $|v|_2;|v|;||v||$ | L2-norm of a vector |
| $T:R^2 \rightarrow R^3; T (u) = w$  | transformation $T $ of a vector $v \in R^2$ into the vector $w \in R^3$ |

# Systems of Equations

## Linear Algebra and Machine Learning

- A common machine learning approach to modeling systems is called Linear Regression

![Untitled](Images/LinearAlgebra/Untitled.png)

- You can make accurate predictions of the output under the assumption that this is a linear relationship

![Untitled](Images/LinearAlgebra/Untitled%201.png)

- You’d find the values of the weight and bias terms that solve all these equations at the same time, From this common machine learning model appears a fundamental  concept of linear algebra called a **system of linear equations**.

![Untitled](Images/LinearAlgebra/Untitled%202.png)

- Linear Algebra is all about manipulating vectors and matrices to do powerful calculations, This math is the backbone of many machine learning models.

![Untitled](Images/LinearAlgebra/Untitled%203.png)

## System of Sentences

- The way sentences combined to give you information is similar to the way equations combined to give you information.

![Untitled](Images/LinearAlgebra/Untitled%204.png)

- When a system is complete is called a **non-singular system**, A **non-singular system** is a system that carries as many pieces of information as sentences, It’s the most informative the system can be.
- A **singular system** is less informative than a non-singular one.

## System of Equations

- Just like with **systems** of **sentences**, **systems** of **linear equations** can also be **singular** or **non-singular** based on how much information they carry.

![Untitled](Images/LinearAlgebra/Untitled%205.png)

![Untitled](Images/LinearAlgebra/Untitled%206.png)

## System of Equations as lines and planes

- Linear equations can easily be visualized as lines in the coordinate plane and systems of linear equations can be represented as arrangements of lines in the plane. This way, you can visualize their solutions and their singularity, or non singularity in a much clearer way.

![Untitled](Images/LinearAlgebra/Untitled%207.png)

- The notion of **slope** and **y-intercept** in a line, The **slope** of a line is a measure of its steepness. Mathematically, slope is calculated as "rise over run”,
- For the line on the **left** is $-1$ as for every unit you move to the right, the line move one unit down.
- For the line on the **right**, the **slope** is $\frac{-1}{2}$  because for every unit you move to the right, line moves half a unit down.

![Untitled](Images/LinearAlgebra/Untitled%208.png)

![Untitled](Images/LinearAlgebra/Untitled%209.png)

- In the same way as a linear equation with two variables correspond to a line in the plane. A linear equation on three variables correspond to a plane in space.
- In the same way as we used to intersect lines to get points as the solutions to the systems equations, you can also intersect planes.

![Untitled](Images/LinearAlgebra/Untitled%2010.png)

![Untitled](Images/LinearAlgebra/Untitled%2011.png)

![Untitled](Images/LinearAlgebra/Untitled%2012.png)

## A Geometric Notion of Singularity

- There is an even simpler way to visualize singularity and non-singularity. It involves slightly simplifying the system. Note that system 2 and 3, the singular ones, are very similar since both consists of parallel lines.
- We can contract system 2 and 3 into one bucket by looking at the constants of the systems of equations, These constants are the numbers in the equations that are not accompanying the variables A or B.

![Untitled](Images/LinearAlgebra/Untitled%2013.png)

- If we turn all these constants for all these three systems into 0 and see what happens to the plots. The new systems always have the 0.00 as a solution, so they must pass by the origin.
- Notice that system 1 is still a pair of intersecting lines, so it still has a unique solution. So it's still complete and non-singular.
- System 2 is still a pair of identical lines, so it still has infinite solutions, so it's redundant and singular.
- System 3 went from a pair of non-intersecting parallel lines to a pair of identical lines. So now it has infinitely many solutions instead of none. It went from contradictory to redundant. However, it stayed singular as before.
- So in conclusion, the constants in the system don't matter when it comes to determine the system is singular or non-singular.
- You can now start considering systems of equations where the constants are always 0, These are much simpler.

![Untitled](Images/LinearAlgebra/Untitled%2014.png)

## **Singular vs non-singular matrices**

- **Matrices** have lots of very important properties and they arise from many different places in math. In our case, They will arise from the **coefficients** in a system's of equations in a very natural way.
- **Matrices** like systems of linear equations can also be singular or non-singular.

![Untitled](Images/LinearAlgebra/Untitled%2015.png)

![Untitled](Images/LinearAlgebra/Untitled%2016.png)

## Linear Dependence and Independence

- A system of equation is singular if the second equation carries the same information as the first one. This is the concept of **linear dependence**.

![Untitled](Images/LinearAlgebra/Untitled%2017.png)

![Untitled](Images/LinearAlgebra/Untitled%2018.png)

![Untitled](Images/LinearAlgebra/Untitled%2019.png)

![Untitled](Images/LinearAlgebra/Untitled%2020.png)

![Untitled](Images/LinearAlgebra/Untitled%2021.png)

![Untitled](Images/LinearAlgebra/Untitled%2022.png)

## The Determinant

- There is a much faster way to tell if a matrix is singular or non-singular, It's called the **determinant** and it is a quick formula that returns a zero if the matrix is singular and a number different from zero if the matrix is non-singular.
- the **determinant** is zero if the matrix is **singular** and non-zero if it's **non-singular.**

![Untitled](Images/LinearAlgebra/Untitled%2023.png)

![Untitled](Images/LinearAlgebra/Untitled%2024.png)

- The determinant for three-by-three matrix is a bit more complicated than that for two-by-two matrix, but it is mostly the same.
- Here are the diagonals in 2 x 2 and 3 x 3 matrices:

![Untitled](Images/LinearAlgebra/Untitled%2025.png)

- For the determinant, you're simply going to add the products of the elements in these diagonals and subtract the products of the elements in these diagonals that go the other way around.

![Untitled](Images/LinearAlgebra/Untitled%2026.png)

![Untitled](Images/LinearAlgebra/Untitled%2027.png)

![Untitled](Images/LinearAlgebra/Untitled%2028.png)

# Solving Systems of Linear Equations: Elimination

## **Solving non-singular system of linear equations**

- we will learn a method that finds the solution to a system of linear equations and can tell you if the system is singular or non-singular.

![Untitled](Images/LinearAlgebra/Untitled%2029.png)

![Untitled](Images/LinearAlgebra/Untitled%2030.png)

![Untitled](Images/LinearAlgebra/Untitled%2031.png)

## Solving singular system of linear equations****

![Untitled](Images/LinearAlgebra/Untitled%2032.png)

![Untitled](Images/LinearAlgebra/Untitled%2033.png)

## **Solving systems of equations with more variables**

![Untitled](Images/LinearAlgebra/Untitled%2034.png)

![Untitled](Images/LinearAlgebra/Untitled%2035.png)

![Untitled](Images/LinearAlgebra/Untitled%2036.png)

## Matrix Row-Reduction

- **Matrix row reduction,** also called the **Gaussian elimination**, consists of applying the exact same manipulations except to the rows of a matrix. In order to turn that matrix into a much more simplified form, from which you can extract lots of useful information.
- From the original matrix, you can get the matrix in the intermediate system by applying some row manipulations. An important feature of this matrix is that it has 1s in the main diagonal and zeros underneath the diagonal and is called **Row Echelon Form.**
- Finally, some more manipulations will get you to the matrix with ones in the diagonal and zeroes everywhere else and is called Reduced **Row Echelon Form**

![Untitled](Images/LinearAlgebra/Untitled%2037.png)

![Untitled](Images/LinearAlgebra/Untitled%2038.png)

![Untitled](Images/LinearAlgebra/Untitled%2039.png)

![Untitled](Images/LinearAlgebra/Untitled%2040.png)

- A matrix in **row echelon form** looks like one of the following:
    - For 3 x 3 matrices:
        - On the main diagonal, we can have bunch of ones followed by perhaps a bunch of zeros.
        - On the main diagonal, we can have all ones.
        - On the main diagonal, we can also have all zeros.
        - Below the diagonal, everything is a zero.
        - To the right of the ones any number is allowed
        - Finally to the right of the zeros, everything must be zero.
    - For 2 x 2 matrices:
        - You can have two 1s in the diagonal
        - You can have one 1s in the diagonal.
        - you can have zero 1s in the diagonal.

![Untitled](Images/LinearAlgebra/Untitled%2041.png)

## **Row operations that preserve singularity**

- The same manipulations that you use to solve systems of linear equations can be used in matrices. These are called **row operations** in a matrix
- A very important property that they have is that they preserve the **singularity** of a matrix. In other words, if you apply them to a **singular matrix**, you get a **singular matrix** and if you apply them to a **non-singular matrix**, you get a **non-singular matrix**.
- The Row Operations that preserve singularity:

![Untitled](Images/LinearAlgebra/Untitled%2042.png)

![Untitled](Images/LinearAlgebra/Untitled%2043.png)

![Untitled](Images/LinearAlgebra/Untitled%2044.png)

# Solving System of Linear Equations: Row Echelon Form and Rank

## The Rank of a Matrix

- **Rank of a matrix**  measures how much information that matrix or its corresponding system linear equations is carrying.
- One great application of ranking machine learning is an **image compression**. The image below uses a lot of storage because every pixel intensity has to be stored as a number.
- We can store this image or perhaps a slightly blurrier version of it using significantly less space. By utilizing rank of a matrix.
- It turns out that pixelated images are matrices and the rank of matrix is related to the amount of space that is needed to store that corresponding image.
- There's a very powerful technique on **singular value decomposition(SVD)**, which can reduce the **rank of a matrix** while changing it as little as possible.
- Notice how the images of rank 15 and 50 are very similar to the original and we take a lot less space to store.

![Untitled](Images/LinearAlgebra/Untitled%2045.png)

- The amount of information a system of sentences carries is defined as the rank of the system.

![Untitled](Images/LinearAlgebra/Untitled%2046.png)

- Rank of the matrix is defined as the rank of the corresponding system of equations.

![Untitled](Images/LinearAlgebra/Untitled%2047.png)

- There's a special relationship between the **rank of a matrix** and its **solution space**. The **solution space** for each matrix is the set of solutions to the system of equations when the **constants** are **zero**.

![Untitled](Images/LinearAlgebra/Untitled%2048.png)

- For the first one the solutions are only a=0 and b=0. So that's a point and the dimensional solution space is 0 because the dimension of a point is 0.
- For the second one, the set of solutions was some line and a line has dimension 1. So the dimension of the solution space was 1.
- For the third one, well every a and b works here because any point a and b is a solution to that system. Therefore the solution space is a plane and it has dimension 2.
- **Rank of matrix** is equal to **number** of **rows** in the matrix minus the dimension of the **solution space**.

![Untitled](Images/LinearAlgebra/Untitled%2049.png)

- A matrix is non-singular if only has full rank, namely if the rank is equal to the number of rows.

![Untitled](Images/LinearAlgebra/Untitled%2050.png)

## The Rank of a Matrix in General

- This is how the Rank works for larger matrices.

![Untitled](Images/LinearAlgebra/Untitled%2051.png)

## Row Echelon Form

- There is a simpler way to calculate the rank using **Row Echelon Form of a matrix.**
- We used the systems of equations to find **Row Echelon Form of a matrix**. We can use the row operations to calculate the **row echelon form.**

![Untitled](Images/LinearAlgebra/Untitled%2052.png)

![Untitled](Images/LinearAlgebra/Untitled%2053.png)

![Untitled](Images/LinearAlgebra/Untitled%2054.png)

- The **rank of a matrix** is the number of **ones** in the diagonal of the **row echelon form**. Also a matrix is **NON-singular** if and only if the **row echelon form** has **only ones** and **no zeros.**

![Untitled](Images/LinearAlgebra/Untitled%2055.png)

## **Row echelon form in general**

![Untitled](Images/LinearAlgebra/Untitled%2056.png)

![Untitled](Images/LinearAlgebra/Untitled%2057.png)

![Untitled](Images/LinearAlgebra/Untitled%2058.png)

![Untitled](Images/LinearAlgebra/Untitled%2059.png)

![Untitled](Images/LinearAlgebra/Untitled%2060.png)

![Untitled](Images/LinearAlgebra/Untitled%2061.png)

## Reduced Row Echelon Form

![Untitled](Images/LinearAlgebra/Untitled%2062.png)

- The way to get from the **row echelon form** matrix to the **reduced row echelon form** is simply to use each 1s in the diagonal to disappear all the non-zero entries above.

![Untitled](Images/LinearAlgebra/Untitled%2063.png)

![Untitled](Images/LinearAlgebra/Untitled%2064.png)

![Untitled](Images/LinearAlgebra/Untitled%2065.png)

![Untitled](Images/LinearAlgebra/Untitled%2066.png)

## **The Gaussian Elimination Algorithm**

- It is just the **elimination** method you studied before but restructured to solve a system of equations and formalize so that it can be followed by hand or implemented in code.
- Recall that when you learn about the singularity of a matrix, you ignore the constant values on the right-hand side of your equations. You treated them as though they were just zero.
- To actually solve a **system of equations**, you will use them. To begin, make a matrix from the coefficient in your system of equations and add another column to the right side of your matrix, which holds the constant values. This is called the **augmented matrix**. The vertical line is used to separate the constants, so you remember they're not part of the variables.
- You can use the augmented matrix to solve your system of equations using the Elimination method.

![Untitled](Images/LinearAlgebra/Untitled%2067.png)

- To complete the elimination method, you'll repeatedly find an element called a **pivot** on the diagonal of the matrix. To start, you'll select the top left cell as your **pivot**. Use row operations to set your **pivot** to **one**. Then use row operations to set all the values below your **pivot** to **zero**. After that repeat the process row by row using row operations to simplify the matrix down to the **reduced row-echelon form**.
- Whatever row operations you perform on the matrix will also be applied to the **column** of **constants** you included to form the **augmented matrix**. They'll eventually help us solve the system of equations

![Untitled](Images/LinearAlgebra/Untitled%2068.png)

- here we reached the **Row Echelon Form,** The diagonal is all ones, and below the diagonal, there's only zeros.

![Untitled](Images/LinearAlgebra/Untitled%2069.png)

- Then we will use the information in constant column to actually solve the **system of equations** through a process called **back substitution**.
- Here's how **back substitution** works, You will start from the bottom row and work your way to the top. You'll use the **pivot** from each row to cancel the values in the cells above it.

![Untitled](Images/LinearAlgebra/Untitled%2070.png)

- Do that until you've got a matrix that has all ones in the diagonal and zeros on all other positions.
- Note that the square part of the **augmented matrix** has only 1s in the diagonal. Such a matrix is called the **identity matrix**. By simplifying the matrix to this form using **Gaussian elimination**, you have solved the original **system** of **equations**.

![Untitled](Images/LinearAlgebra/Untitled%2071.png)

- Will Gaussian elimination work if the matrix is singular?
    - We know that if the matrix is singular, then in reduced **row-echelon form**, you'll have a row that is all zeros. Once you get here, the algorithm just stops.
    - The whole point of **Gaussian elimination** is to find solutions to a **system** of **equations**. If you find a row of zero, you know your matrix is singular and there is no solution.
    
    ![Untitled](Images/LinearAlgebra/Untitled%2072.png)
    
    - You can still determine if your matrix is **contradictory** and has **no solutions** or if it has **infinitely** many **solutions**.
    - To do that, you just need to look at the **column** of **constants**. If the **constant** value in the **row** of **zero** is also **zero**, the row just says 0a+0b+0c = 0. No matter what values you choose for A, B, and C, the left side will always equal zero, and this equation will be true. The system has **infinitely many solutions.**
    
    ![Untitled](Images/LinearAlgebra/Untitled%2073.png)
    
    - if  you get a matrix with the **constant value** in the **third** row changing to say four. Now the last row states that 0a+0b+0c = 4. No matter what values of A, B, and C you choose, the left side of the equation will equal zero, but the right side equals four. This means that the system h**as no possible solutions**.
    
    ![Untitled](Images/LinearAlgebra/Untitled%2074.png)


## Gaussian Elimination Algorithm Wrap Up

Gaussian elimination offers a systematic approach to solving systems of linear equations by transforming an augmented matrix into row-echelon form, thereby enabling the determination of variables. The algorithm comprises several essential steps:

### Step 1: Augmented Matrix

Consider a system of linear equations:

$$
\begin{align*}
2x_1 + 3x_2 + 5x_3&= 12 \\
-3x_1 - 2x_2 + 4x_3 &= -2 \\
x_1 + x_2 - 2x_3  &= 8 \\
\end{align*}
$$

Create the augmented matrix ([A | B]), where (A) represents the coefficient matrix and (B) denotes the column vector of constants:

$$
A = \begin{bmatrix}
\phantom{-}2 & \phantom{-}3 & \phantom{-}5  \\
-3 & -2 & \phantom-4 \\
\phantom{-}1 & \phantom{-}1 & -2  \\
\end{bmatrix}
$$

$$
B = \begin{bmatrix}
\phantom-12 \\ -2 \\ \phantom-8 
\end{bmatrix}
$$

Thus, \([A | B]\) is represented as:

$$
\begin{bmatrix}
\phantom{-}2 & \phantom{-}3 & \phantom{-}5 & | & \phantom{-}12 \\
-3 & -2 & \phantom-4 & | & -2 \\
\phantom{-}1 & \phantom{-}1 & -2 & | & \phantom{-}8 \\
\end{bmatrix}
$$


### Step 2: Transform Matrix into Reduced Row Echelon Form
Initiate row operations to convert the augmented matrix into row-echelon form. The objective is to introduce zeros below the leading diagonal.

- **Row Switching:** Rearrange rows to position the leftmost non-zero entry at the top.
- **Row Scaling:** Multiply a row by a non-zero scalar.
- **Row Replacement:** Substitute a row with the sum of itself and a multiple of another row.

### Step 3: Solution Check

Examine for a row of zeros in the square matrix (excluding the augmented part).

Consider the following cases:

1. If no row comprises zeros, a **unique solution** exists.
2. If one row contains zeros with a non-zero augmented part, the system has **no solutions**.
3. If every row of zeros has a zero augmented part, the system boasts **infinitely many solutions**.


### Step 5: Back Substitution

After attaining the reduced row-echelon form, solve for variables starting from the last row and progressing upwards.

Remember, the aim is to simplify the system for easy determination of solutions!

import os

# Directory containing the files
directory = '/path/to/your/directory'

# temp

# Vector Algebra

## Vectors and their Properties

- we’ve learned matrices as a race of numbers. A vector is a simpler way of number that only has one column. It turns out that vectors can be seen as arrows in the plane or in a higher dimensional space.
- Two very important components of vectors are their magnitude, and their direction.
- The number of coordinates in the vector is the dimension of the space in which it lives.

![Untitled](Images/LinearAlgebra/Untitled%2075.png)

- To find the magnitude or size of a vector, There’s two ways to calculate it using **L1 Norm or L2 Norm.**
- We are using absolute value because vector can be in any directions but distance can only be positive.

![Untitled](Images/LinearAlgebra/Untitled%2076.png)

- By default, when you specify with norm to use, we are using the L2 norm. The reason is that it's a more natural one because precisely it is the length of the arrow.

![Untitled](Images/LinearAlgebra/Untitled%2077.png)

- The direction of a vector can also be deduced from its coordinates.

![Untitled](Images/LinearAlgebra/Untitled%2078.png)

## **Sum and difference of vectors**

![Untitled](Images/LinearAlgebra/Untitled%2079.png)

![Untitled](Images/LinearAlgebra/Untitled%2080.png)

![Untitled](Images/LinearAlgebra/Untitled%2081.png)

## **Distance between vectors**

- The difference between two vectors is helpful to tell how far apart two vectors are from each other.

![Untitled](Images/LinearAlgebra/Untitled%2082.png)

## **Multiplying a vector by a scalar**

![Untitled](Images/LinearAlgebra/Untitled%2083.png)

![Untitled](Images/LinearAlgebra/Untitled%2084.png)

## **The Dot Product**

- The **dot product** is a very nice and compact way to express systems of linear equations using matrices and vectors.

![Untitled](Images/LinearAlgebra/Untitled%2085.png)

![Untitled](Images/LinearAlgebra/Untitled%2086.png)

- The **L2 norm** is always the **square root** of the **dot product** between the vector and itself.

![Untitled](Images/LinearAlgebra/Untitled%2087.png)

- A **Transpose** essentially transforms columns into rows, denoted by superscript $T$.

![Untitled](Images/LinearAlgebra/Untitled%2088.png)

![Untitled](Images/LinearAlgebra/Untitled%2089.png)

![Untitled](Images/LinearAlgebra/Untitled%2090.png)

![Untitled](Images/LinearAlgebra/Untitled%2091.png)

## Geometric Dot Product

- The angle between two vectors is very important. There are some nice relations between the angle and the **dot product**.
- A two vectors are orthogonal if and only if the **dot product** is zero.

![Untitled](Images/LinearAlgebra/Untitled%2092.png)

![Untitled](Images/LinearAlgebra/Untitled%2093.png)

- you can tell if the dot product between two vectors is positive, negative, or zero. The sign of the dot product of a vector corresponds to being on one side or the other one off that perpendicular vector.

![Untitled](Images/LinearAlgebra/Untitled%2094.png)

## **Multiplying a matrix by a vector**

![Untitled](Images/LinearAlgebra/Untitled%2095.png)

# Linear Transformations

## Matrices as Linear Transformations

- Another very powerful and very useful representation of matrix is called a **linear transformation**.
- **A linear transformation** is a way to send each point in the plane into another point in the plane in a very structured way.
- The square on the left is called a **basis**. And so is the parallelogram on their right.

![Untitled](Images/LinearAlgebra/Untitled%2096.png)

- A very special property that **basis** have is that they cover the entire plane. Since this square actually tessellates the whole plane and the parallelogram tessellates the whole plane as well. Then the linear transformation is simply defined as a change of coordinates.

![Untitled](Images/LinearAlgebra/Untitled%2097.png)

## Linear Transformations as Matrices

- How to start with a **linear transformation**, and then find the corresponding matrix?
    - Let’s say we have the basis matrix and it sends to the one on the right.
    
    ![Untitled](Images/LinearAlgebra/Untitled%2098.png)
    
    - We only two points (1,0) and (0,1) to get the original matrix.
    - The matrix that sends the vector (1,0)  to (3,-1) is the first column and (0,1) to (2,3) is the second column.
    - That is how you turn a **linear transformation** into its corresponding matrix. You only look at where the two fundamental vectors (1,0) and (0,1) go, and those are your columns of the matrix.
    
    ![Untitled](Images/LinearAlgebra/Untitled%2099.png)
    

## Matrix Multiplication

- **Matrix multiplication** corresponds to combining two **linear transformations** into a third one.
- How to apply **matrix multiplication** using **linear transformation?**
    - Let’s start with a matrix  $\begin{matrix} 3 & 1 \\ 1 & 2 \end{matrix}$  and apply **linear transformation** to get from the fundamental basis on the left to the one one the right.
    
    ![Untitled](Images/LinearAlgebra/Untitled%20100.png)
    
    - Then we take the basis matrix on the right $\begin{matrix} 3 & 1 \\ 1 & 2 \end{matrix}$  and apply **linear transformation** correspond to the matrix  $\begin{matrix} 2 & -1 \\ 0 & 2 \end{matrix}$ , The parallelogram on the left turns into this parallelogram on the right.
    
    ![Untitled](Images/LinearAlgebra/Untitled%20101.png)
    
    - By combining all **linear transformations together,** you will find a **linear transformation** between the first and the third that correspond to some matrix.
    
    ![Untitled](Images/LinearAlgebra/Untitled%20102.png)
    
    - By looking at only the basis vectors, we can find the **linear transformation.**
    
    ![Untitled](Images/LinearAlgebra/Untitled%20103.png)
    
- We can obtain the matrix from the first to the third from the other two matrices using **Matrix Multiplication,** Operation over here is matrix on the left times matrix on the right equals the third matrix.
- Notice that the matrix got flipped and The reason is because the **linear transformations** act on the vector on the left, so you multiply matrix times vector.

![Untitled](Images/LinearAlgebra/Untitled%20104.png)

![Untitled](Images/LinearAlgebra/Untitled%20105.png)

![Untitled](Images/LinearAlgebra/Untitled%20106.png)

## The Identity Matrix

- The **identity matrix** is the matrix that when multiplied by any other matrix it gives the same matrix.
- The corresponding linear transformation is very simple. It is the one that leaves the plane intact.

![Untitled](Images/LinearAlgebra/Untitled%20107.png)

## Matrix Inverse

- A **matrix** can have a very special matrix associated with it called the **inverse**.
- The **inverse matrix** is precisely that matrix for which the **product** of the matrices is the **identity matrix**.
- In a **linear transformation**, the **inverse** matrix is the one that undoes the job of the original matrix, namely the one that returns the plane to where it was at the beginning.

![Untitled](Images/LinearAlgebra/Untitled%20108.png)

- How to find the Entries of the **inverse matrix?**
    - Using this formula: $\begin{bmatrix} a & b\\ c & d\end{bmatrix} ^ {-1} = \frac{1}{ad - bc} * \begin{bmatrix} d & -b\\ -c & a\end{bmatrix}$   where  $\frac{1}{ad - bc}$ is the **determinant.**
    
    ![Untitled](Images/LinearAlgebra/Untitled%20109.png)
    
    - By solving a **system of linear equations**
    
    ![Untitled](Images/LinearAlgebra/Untitled%20110.png)
    
- Some matrices have no **inverse.**

![Untitled](Images/LinearAlgebra/Untitled%20111.png)

## Which Matrices have an Inverse

![Untitled](Images/LinearAlgebra/Untitled%20112.png)

## Neural Networks and Matrices

- **Neural networks**, one of the most successful and powerful machine learning models out there with numerous applications, is based largely on matrices and matrix products.

![Untitled](Images/LinearAlgebra/Untitled%20113.png)

![Untitled](Images/LinearAlgebra/Untitled%20114.png)

![Untitled](Images/LinearAlgebra/Untitled%20115.png)

![Untitled](Images/LinearAlgebra/Untitled%20116.png)

![Untitled](Images/LinearAlgebra/Untitled%20117.png)

# Determinants In-Depth

## Machine Learning Motivation

![Untitled](Images/LinearAlgebra/Untitled%20118.png)

![Untitled](Images/LinearAlgebra/Untitled%20119.png)

## Singularity and Rank of Linear Transformation

- Since matrices correspond to **linear transformations**, then **linear transformations** can also be **singular** and **non-singular.**
- A matrix  that sends the whole plane to the whole plane is non-singular.
- A matrix that sends the whole plane to simply a line is singular
- A matrix that sends the whole plane to a point is also singular.
- Notice that the dimension of the image is precisely the rank of this matrix.
- The dimension of the image of the linear transformation is another way to calculate **rank of the matrix.**

![Untitled](Images/LinearAlgebra/Untitled%20120.png)

## Determinant as an Area

- in **linear transformation**, the **determinant** is explained as an area, or as a volume. When this area or volume is 0, then the matrix is **singular**.

![Untitled](Images/LinearAlgebra/Untitled%20121.png)

- A parallelogram can have a negative **area** depending on what order we take the two **basis** vectors, It Doesn’t affect the singularity of the matrix.
- The area of the parallelogram is negative, if we take the vectors in counterclockwise order and positive if we take them in clockwise order.

![Untitled](Images/LinearAlgebra/Untitled%20122.png)

## Determinant of a Product

![Untitled](Images/LinearAlgebra/Untitled%20123.png)

![Untitled](Images/LinearAlgebra/Untitled%20124.png)

- If you consider the combination of a two linear transformations
    - first, you're blowing up the area of the fundamental basis by **determinant.**
    - Then blowing up the area of the resulted **basis** by the second matrix **determinant**.
    - So you're blowing them up by **1st det** times **2nd det** which is what this transformation does.

![Untitled](Images/LinearAlgebra/Untitled%20125.png)

![Untitled](Images/LinearAlgebra/Untitled%20126.png)

## Determinants of Inverses

![Untitled](Images/LinearAlgebra/Untitled%20127.png)

![Untitled](Images/LinearAlgebra/Untitled%20128.png)

![Untitled](Images/LinearAlgebra/Untitled%20129.png)

# Eigenvalues and Eigenvectors

## Basis in Linear Algebra

- The main property of a **basis** is that every point in the space can be expressed as a linear combination of elements in the **basis**.
- We pick any point in space, and get to that point only walking in the two directions defined by the **basis.**

![Untitled](Images/LinearAlgebra/Untitled%20130.png)

- any two vectors that go in the same direction and they could be opposites or go in the same direction, as long as they belong to the same line, the two vectors do not form a **basis**.

![Untitled](Images/LinearAlgebra/Untitled%20131.png)

## Span in Linear Algebra

- Some vectors cannot be a **basis** of a plane but still be a basis for some other space.
- The **span** of a set of **vectors** is simply the set of **points** that can be reached by walking in the direction of these vectors in any combination.
- The **span** of these two vectors is the **plane**.

![Untitled](Images/LinearAlgebra/Untitled%20132.png)

- These two vectors don’t span the plane, but span that line.

![Untitled](Images/LinearAlgebra/Untitled%20133.png)

- To say that vectors form a **basis** of a line or a plane it needs to be a minimal spanning set.
- In the Example below, Each vector **spans** the line, but in order to be a **basis** it needs to be a minimal spanning set. On the other hand each one of them separately is a **basis** of that line.

![Untitled](Images/LinearAlgebra/Untitled%20134.png)

- In the Example below, This vector **spans** the line and also is a **basis** of that line.

![Untitled](Images/LinearAlgebra/Untitled%20135.png)

- In the Example below:
    - The Two vectors on the left spans the plane and form a basis of that plane
    - The Two vectors on the right spans the plane but don’t form a basis of that plane because a basis is a minimal spanning set and the 3rd vector is redundant.

![Untitled](Images/LinearAlgebra/Untitled%20136.png)

- Notice that the length of the **basis** is the space of the **dimension** of the plane.

![Untitled](Images/LinearAlgebra/Untitled%20137.png)

- A group of vectors is said to be **linearly independent** if **none** of the vectors in the group can be obtained as a linear combination of the others.
- If a vector can be obtained as a linear combination of other vectors, this set of vectors is called **linearly dependent.**

![Untitled](Images/LinearAlgebra/Untitled%20138.png)

![Untitled](Images/LinearAlgebra/Untitled%20139.png)

- Let’s say you have 3 vectors $v_{1},v_{2},v_{3}$  To Check for linear independence, you will find the constants $\alpha$ and $\beta$ that satisfy $\alpha * v_{1} + \beta * v_{2} = v_{3}$
- If you find a **solution** for the system of equations, then $v_{3}$  is a linear combination of $v_{1}$ and $v_{2}$ , and this set is **linearly dependent**.
- If you find that the system has **no solution**, then that means the set is **linearly independent**.
- Note that if you have a 3d plane for example and you have two vectors that are **linearly dependent**, They are not a **basis** for this 3d space.

![Untitled](Images/LinearAlgebra/Untitled%20140.png)

## Eigenbasis

- There are some basis that are more useful than others and it is called the **eigenbasis**.
- If we choose a different basis with (1,0) and (1,1) vectors, The new basis forms a parallelogram and the sides of the two parallelograms are parallel to the corresponding one in the other basis.

![Untitled](Images/LinearAlgebra/Untitled%20141.png)

- Because these sides are parallel, then what we're doing to the plane is we're stretching the plane.
- **Eignenbasis** is a very special basis that only consists of two stretching and gives a way of looking at a **linear transformation** with respect to a basis that sends a parallelogram to another parallelogram with sides parallel to the original one.

![Untitled](Images/LinearAlgebra/Untitled%20142.png)

- Eigenbasis is useful because it simplifies the linear transformation by stretching the horizontal vector and the vertical vector.
- The two vectors in the **basis** are gonna be called the **eigenvectors** and the **stretching factor** are going to be called **eigenvalues**.

## Eigenvalues and Eigenvectors

- Given a matrix $\begin{bmatrix} 2 & 1\\ 0 & 3\end{bmatrix}$, two **eigenvectors (1,0), (1,1)** and a third vectors that isn’t. ****
- When multiplying a matrix by **eigenvector,** You could write the result like **stretching factor(eigenvalue)** times **eigenvector**
- On the other hand, after multiplying the third vector, it’s no longer pointing in the same direction and there's no constant that I can multiply the original vector by to get the final vector 0,6

![Untitled](Images/LinearAlgebra/Untitled%20143.png)

- Notice that **eigenvectors** and **eigenvalues** come in pairs, so $v_{1}$ and $\lambda_1$  form a pair and $v_{2}$ and $\lambda_2$  form a second pair.

![Untitled](Images/LinearAlgebra/Untitled%20144.png)

- The reason why eigenvectors are important is you can turn a matrix multiplication which needs a lot of work into a scalar multiplication which is less work.

![Untitled](Images/LinearAlgebra/Untitled%20145.png)

- If we apply what we know about basis, We can use this shortcut given by the eigenvectors everywhere.
- For example:
    - The red vector is not an eigenvector, however, The two **eigen vectors (**1,0) and (1,1) are **linearly independent** and **span** the plane, so they form a **basis**.
    - Also This is actually the eigen basis of the matrix $\begin{bmatrix} 2 & 1\\ 0 & 3\end{bmatrix}$.
    - Instead of matrix multiplication, We can write the equation by writing the vector (-1,2) as a **linear combination** of the vectors in the **basis.**
    - This way you turned a matrix multiplication into a scalar multiplication
    
    ![Untitled](Images/LinearAlgebra/Untitled%20146.png)
    
    ![Untitled](Images/LinearAlgebra/Untitled%20147.png)
    
    ![Untitled](Images/LinearAlgebra/Untitled%20148.png)
    
    ![Untitled](Images/LinearAlgebra/Untitled%20149.png)
    
- There is one setback and that there’s a substantial work involved in getting the red vector coordinated with respect to the **eigenbasis**

![Untitled](Images/LinearAlgebra/Untitled%20150.png)

- A more accurate way of describing **eigenvectors** is not that they remove all the work, but they let you decide when you want to do the work.

![Untitled](Images/LinearAlgebra/Untitled%20151.png)

## Calculating Eigenvalues and Eigenvectors

- Notice that these two transformations are not the same transformation, but they do coincide in many points.
- In other words, they act the exact same way for infinitely many points, all the points in this line.

![Untitled](Images/LinearAlgebra/Untitled%20152.png)

- In this diagonal, the two transformations do the exact same thing, so they match in infinitely many points.
- **Transformations** should only match at one point (0,00, when they match at infinitely many points, something **non-singular** is happening.
- If these two **transformations** match at infinitely many points, that means their difference is 0 at infinitely many points.

![Untitled](Images/LinearAlgebra/Untitled%20153.png)

- Finding **eigenvalues** and **eigenvectors** of 2x2 matrix

![Untitled](Images/LinearAlgebra/Untitled%20154.png)

![Untitled](Images/LinearAlgebra/Untitled%20155.png)

- Finding **eigenvalues** and **eigenvectors** of 2x2 matrix

![Untitled](Images/LinearAlgebra/Untitled%20156.png)

![Untitled](Images/LinearAlgebra/Untitled%20157.png)

![Untitled](Images/LinearAlgebra/Untitled%20158.png)

- Note that there is infinite solutions of **eigenvectors**
- Also you can only find **eigenvalues** and **eigenvectors** only in square matrix. ****

![Untitled](Images/LinearAlgebra/Untitled%20159.png)

## On the Number of Eigenvectors

![Untitled](Images/LinearAlgebra/Untitled%20160.png)

![Untitled](Images/LinearAlgebra/Untitled%20161.png)

![Untitled](Images/LinearAlgebra/Untitled%20162.png)

![Untitled](Images/LinearAlgebra/Untitled%20163.png)

![Untitled](Images/LinearAlgebra/Untitled%20164.png)

![Untitled](Images/LinearAlgebra/Untitled%20165.png)

## Dimensionality Reduction and Projection

![Untitled](Images/LinearAlgebra/Untitled%20166.png)

- The idea behind **dimensionality reduction** is to move your data points into a vector space with fewer dimensions, this is called a **projection**.
- How **projection** works?
    - For Example, you have data in 2 dimensions x and y, You want to project your data onto a line with 1 dimension which has equation y=x.
    - All the points move perpendicularly towards the line, except the point (1,1) which was already on the line
    - To calculate where these points end up on the line, we can give each point's location as one coordinate its distance along the line from origin.
    - The location comes from the **dot product** of the coordinates of each point with some other vector shown here in orange.
    - To choose that vector, In our case the line **y = x** is actually the span of the vector with coordinates (1,1), To find the new location projected along the line.
    - Then we divide by the vector's norm to ensure that there is no stretching introduced.
    
    ![Untitled](Images/LinearAlgebra/Untitled%20167.png)
    
    ![Untitled](Images/LinearAlgebra/Untitled%20168.png)
    
    - You now need only one column vector instead of a two column matrix in order to represent your points locations along these line.

![Untitled](Images/LinearAlgebra/Untitled%20169.png)

![Untitled](Images/LinearAlgebra/Untitled%20170.png)

## Motivating PCA

- How is the concept of **projection** is used by **PCA** to reduce the dimensions of the dataset?
    - given the graph below, Each dot represents a different observation composed of two features x and y.
    - Reducing the dimensions of this data means moving from two dimensional data in a plane to one dimensional data along a line.
    - We will start by centering the data around the **origin (0,0).**
    - After trying multiple projections, the data points may be more or less spread out on the line.
    - The spreadness of the data is very important, The more spread our data points, The more we preserve more information from the original dataset.

![Untitled](Images/LinearAlgebra/Untitled%20171.png)

- The goal of **PCA** will be to find the projection that preserves the maximum possible spread in your data, even as you reduce the dimensionality of your dataset

![Untitled](Images/LinearAlgebra/Untitled%20172.png)

## Variance and Covariance

- PCA relies on a few concepts from statistics
- The **mean** of your data is simply the average value of all the observations

![Untitled](Images/LinearAlgebra/Untitled%20173.png)

- The **variance**, which describes how spread out your data is.

![Untitled](Images/LinearAlgebra/Untitled%20174.png)

![Untitled](Images/LinearAlgebra/Untitled%20175.png)

![Untitled](Images/LinearAlgebra/Untitled%20176.png)

![Untitled](Images/LinearAlgebra/Untitled%20177.png)

- There some situations where variance alone wouldn’t be helpful.
- The two datasets below have three observations each. They would have identical y variance and x variance.
- It's obvious that there's a significant difference in the patterns of these datasets.
- The solution is now a measure called **covariance, Covariance** helps measure how two features of a dataset varies and with respect to one another.

![Untitled](Images/LinearAlgebra/Untitled%20178.png)

- The equation for **covariance** looks similar to the **variance** equation.

![Untitled](Images/LinearAlgebra/Untitled%20179.png)

![Untitled](Images/LinearAlgebra/Untitled%20180.png)

## Covariance Matrix

- We learnt about **variance** and **covariance** to build a special matrix called the **covariance matrix**.
- It is a compact way of storing all the relationships between pairs of variables in your data set.
- It simply store the **covariances** and **variances** for each pair of variables.

![Untitled](Images/LinearAlgebra/Untitled%20181.png)

- You first calculate the **variance** of each variable and the **covariance** of each combination of variables.
- Then build a square matrix with a row and column for each variable in your data set.
- At every position in your matrix you place the **covariances** of the variables of that row and column.
- Along the main diagonal, you place the variance of each variable.
- Notice that the covariance of (x, y) is the same as the covariance of (y, x).
- Also the covariance of a variable with itself is actually just the variance of this variable.

![Untitled](Images/LinearAlgebra/Untitled%20182.png)

![Untitled](Images/LinearAlgebra/Untitled%20183.png)

![Untitled](Images/LinearAlgebra/Untitled%20184.png)

![Untitled](Images/LinearAlgebra/Untitled%20185.png)

![Untitled](Images/LinearAlgebra/Untitled%20186.png)

![Untitled](Images/LinearAlgebra/Untitled%20187.png)

## PCA - Overview

![Untitled](Images/LinearAlgebra/Untitled%20188.png)

- **PCA** steps:
    - Center the data around the mean point.
    - Find the **Covariance matrix.**
    - Then find the **eigenvalues** and **eigenvectors** of the **covariance matrix,** with the **eigenvector** giving a **direction** and the **eigenvalues** giving a **magnitude,** This is how you'll find the line onto which you should project the data
    - Notice that these two **eigenvectors** are at a 90 degree angle to one another, This is true for the **eigenvectors** of every matrix that is symmetric around its diagonal.
    - Every **covariance matrix** is symmetric, so the eigenvectors you calculate will always be orthogonal.
    - The two **eigenvectors** are called in **PCA**, **principal components.**
    - The **eigenvector** with the largest **eigenvalue** will always be the one that will give the greatest **variance** when you **project** your data.
    - To project your data, create a new matrix where each column is one of the **eigenvectors** scaled by its own **norm**.
    - Finally, multiply the matrices to project your data onto these vectors, giving you your final dataset which has reduced features.
    
    ![Untitled](Images/LinearAlgebra/Untitled%20189.png)
    
    ![Untitled](Images/LinearAlgebra/Untitled%20190.png)
    
    ![Untitled](Images/LinearAlgebra/Untitled%20191.png)
    

## PCA - Why it Works

- Your **covariance matrix C** characterizes the spread of your data.
- The matrix **C's eigenvectors** tells you the direction in which the matrix can be viewed as just stretching.
- The largest **eigenvalue** tells you where that stretching is greatest, and any other direction will be stretched out less.
- Choosing the **eigenvectors** with the biggest **eigenvalues** will give you the directions with the biggest stretch or the most variance.

![Untitled](Images/LinearAlgebra/Untitled%20192.png)

## PCA - Mathematical Formulation

![Untitled](Images/LinearAlgebra/Untitled%20193.png)

![Untitled](Images/LinearAlgebra/Untitled%20194.png)

![Untitled](Images/LinearAlgebra/Untitled%20195.png)